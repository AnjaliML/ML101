{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch TensorBoard Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we Start, to run this tutorial, as also used above, we’ll need to have PyTorch, TorchVision, Matplotlib, and TensorBoard.\n",
    "\n",
    "With conda:\n",
    "conda install pytorch torchvision -c pytorch\n",
    "conda install matplotlib tensorboard\n",
    "\n",
    "With pip:\n",
    "pip install torch torchvision matplotlib tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we’ll be training a variant of LeNet-5 against the Fashion-MNIST dataset. Fashion-MNIST is a set of image tiles depicting various garments, with ten class labels indicating the type of garment depicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch model and training necessities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Image datasets and image manipulation\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Image display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# In case you are using an environment that has TensorFlow installed,\n",
    "# such as Google Colab, uncomment the following code to avoid\n",
    "# a bug with saving embeddings to your TensorBoard directory\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorboard as tb\n",
    "# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing Images in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by adding sample images from our dataset to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather datasets and prepare them for consumption\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Store separate training and validations splits in ./data\n",
    "training_set = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                              batch_size=4,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=2)\n",
    "\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set,\n",
    "                                                batch_size=4,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=2)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# Extract a batch of 4 images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used TorchVision and Matplotlib to create a visual grid of a minibatch of our input data. Below, we use the add_image() call on SummaryWriter to log the image for consumption by TensorBoard, and we also call flush() to make sure it’s written to disk right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default log_dir argument is \"runs\" - but it's good to be specific\n",
    "# torch.utils.tensorboard.SummaryWriter is imported above\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')\n",
    "\n",
    "# Write image data to TensorBoard log dir\n",
    "writer.add_image('Four Fashion-MNIST Images', img_grid)\n",
    "writer.flush()\n",
    "\n",
    "# To view, start TensorBoard on the command line with:\n",
    "#   tensorboard --logdir=runs\n",
    "# ...and open a browser tab to http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start TensorBoard at the command line and open it in a new browser tab (usually at localhost:6006), you should see the image grid under the IMAGES tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing scalars to Visualize Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is useful for tracking the progress and efficacy of your training. Below, we’ll run a training loop, track some metrics, and save the data for TensorBoard’s consumption.\n",
    "\n",
    "Let’s define a model to categorize our image tiles, and an optimizer and loss function for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a single epocg, and evaluate the training vs. validation set losses every 1000 batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "Batch 1000\n",
      "Batch 2000\n",
      "Batch 3000\n",
      "Batch 4000\n",
      "Batch 5000\n",
      "Batch 6000\n",
      "Batch 7000\n",
      "Batch 8000\n",
      "Batch 9000\n",
      "Batch 10000\n",
      "Batch 11000\n",
      "Batch 12000\n",
      "Batch 13000\n",
      "Batch 14000\n",
      "Batch 15000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_loader))\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        # basic training loop\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # Every 1000 mini-batches...\n",
    "            print('Batch {}'.format(i + 1))\n",
    "            # Check against the validation set\n",
    "            running_vloss = 0.0\n",
    "\n",
    "            # In evaluation mode some model specific operations can be omitted eg. dropout layer\n",
    "            net.train(False) # Switching to evaluation mode, eg. turning off regularisation\n",
    "            for j, vdata in enumerate(validation_loader, 0):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = net(vinputs)\n",
    "                vloss = criterion(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "            net.train(True) # Switching back to training mode, eg. turning on regularisation\n",
    "\n",
    "            avg_loss = running_loss / 1000\n",
    "            avg_vloss = running_vloss / len(validation_loader)\n",
    "\n",
    "            # Log the running loss averaged per batch\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                            epoch * len(training_loader) + i)\n",
    "\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- open TensorBoard and have a look at the SCALARS tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard can also be used to examine the data flow within our model. To do this, call the add_graph() method with a model and sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, grab a single mini-batch of images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# add_graph() will trace the sample input through your model,\n",
    "# and render it as a graph.\n",
    "writer.add_graph(net, images)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we switch over to TensorBoard, we see a GRAPHS tab. Double-click the “NET” node to see the layers and data flow within your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize our Dataset With Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 28-by-28 image tiles we’re using can be modeled as 784-dimensional vectors (28 * 28 = 784). It can be instructive to project this to a lower-dimensional representation. The add_embedding() method will project a set of data onto the three dimensions with highest variance, and display them as an interactive 3D chart. The add_embedding() method does this automatically by projecting to the three dimensions with highest variance.\n",
    "\n",
    "Below, we’ll take a sample of our data, and generate such an embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# Select a random subset of data and corresponding labels\n",
    "def select_n_random(data, labels, n=100):\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# Extract a random subset of data\n",
    "images, labels = select_n_random(training_set.data, training_set.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[label] for label in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we switch to TensorBoard and select the PROJECTOR tab, you should see a 3D representation of the projection. You can rotate and zoom the model. Examine it at large and small scales, and see whether you can spot patterns in the projected data and the clustering of labels.\n",
    "\n",
    "For better visibility, it’s recommended to:\n",
    "\n",
    "Select “label” from the “Color by” drop-down on the left.\n",
    "\n",
    "Toggle the Night Mode icon along the top to place the light-colored images on a dark background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we’ve discussed and demonstrated:\n",
    "\n",
    "- Building models with the neural network layers and functions of the torch.nn module\n",
    "\n",
    "- The mechanics of automated gradient computation, which is central to gradient-based model training\n",
    "\n",
    "- Using TensorBoard to visualize training progress and other activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we’ll be adding some new tools to our inventory:\n",
    "\n",
    "- We’ll get familiar with the dataset and dataloader abstractions, and how they ease the process of feeding data to your model during a training loop\n",
    "\n",
    "- We’ll discuss specific loss functions and when to use them\n",
    "\n",
    "- We’ll look at PyTorch optimizers, which implement algorithms to adjust model weights based on the outcome of a loss function\n",
    "\n",
    "Finally, we’ll pull all of these together and see a full PyTorch training loop in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Dataset and DataLoader classes encapsulate the process of pulling your data from storage and exposing it to your training loop in batches.\n",
    "\n",
    "- The Dataset is responsible for accessing and processing single instances of data.\n",
    "\n",
    "- The DataLoader pulls instances of data from the Dataset (either automatically or with a sampler that you define), collects them in batches, and returns them for consumption by your training loop. The DataLoader works with all kinds of datasets, regardless of the type of data they contain.\n",
    "\n",
    "- For this tutorial, we’ll be using the Fashion-MNIST dataset provided by TorchVision. We use torchvision.transforms.Normalize() to zero-center and normalize the distribution of the image tile content, and download both training and validation data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as always, let's visualize the data as a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trouser  Dress  Sneaker  Shirt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "The model we'll use in this example is a variant of LeNet-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we’ll be using a cross-entropy loss. For demonstration purposes, we’ll create batches of dummy output and label values, run them through the loss function, and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6807, 0.5849, 0.2688, 0.1821, 0.9866, 0.5136, 0.5702, 0.4336, 0.8555,\n",
      "         0.8894],\n",
      "        [0.5602, 0.9671, 0.8628, 0.1546, 0.0499, 0.0727, 0.0642, 0.1060, 0.9479,\n",
      "         0.2124],\n",
      "        [0.2872, 0.7421, 0.1496, 0.0931, 0.4349, 0.1538, 0.1584, 0.6515, 0.5248,\n",
      "         0.5301],\n",
      "        [0.0883, 0.8754, 0.6656, 0.3556, 0.6484, 0.1871, 0.8350, 0.3788, 0.8915,\n",
      "         0.3151]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.5353124141693115\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we’ll be using simple stochastic gradient descent with momentum.\n",
    "\n",
    "- It can be instructive to try some variations on this optimization scheme:\n",
    "\n",
    "- Learning rate determines the size of the steps the optimizer takes. What does a different learning rate do to the your training results, in terms of accuracy and convergence time?\n",
    "\n",
    "- Momentum nudges the optimizer in the direction of strongest gradient over multiple steps. What does changing this value do to your results?\n",
    "\n",
    "- Try some different optimization algorithms, such as averaged SGD, Adagrad, or Adam. How do your results differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have a function that performs one training epoch. It enumerates data from the DataLoader, and on each pass of the loop does the following:\n",
    "\n",
    "- Gets a batch of training data from the DataLoader\n",
    "\n",
    "- Zeros the optimizer’s gradients\n",
    "\n",
    "- Performs an inference - that is, gets predictions from the model for an input batch\n",
    "\n",
    "- Calculates the loss for that set of predictions vs. the labels on the dataset\n",
    "\n",
    "- Calculates the backward gradients over the learning weights\n",
    "\n",
    "- Tells the optimizer to perform one learning step - that is, adjust the model’s learning weights based on the observed gradients for this batch, according to the optimization algorithm we chose\n",
    "\n",
    "- It reports on the loss for every 1000 batches.\n",
    "\n",
    "Finally, it reports the average per-batch loss for the last 1000 batches, for comparison with a validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Epoch Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things we’ll want to do once per epoch:\n",
    "\n",
    "- Perform validation by checking our relative loss on a set of data that was not used for training, and report this\n",
    "\n",
    "- Save a copy of the model\n",
    "\n",
    "- Here, we’ll do our reporting in TensorBoard. This will require going to the command line to start TensorBoard, and opening it in another browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 1.7469194574281572\n",
      "  batch 2000 loss: 0.8529754317477345\n",
      "  batch 3000 loss: 0.7080163355637342\n",
      "  batch 4000 loss: 0.6576025178711862\n",
      "  batch 5000 loss: 0.5957352634742856\n",
      "  batch 6000 loss: 0.5675831498454791\n",
      "  batch 7000 loss: 0.5247063276956323\n",
      "  batch 8000 loss: 0.5007821309112478\n",
      "  batch 9000 loss: 0.5173502513158601\n",
      "  batch 10000 loss: 0.4557679505177075\n",
      "  batch 11000 loss: 0.4501024211420299\n",
      "  batch 12000 loss: 0.436078433178569\n",
      "  batch 13000 loss: 0.4188414504616521\n",
      "  batch 14000 loss: 0.44215710255887825\n",
      "  batch 15000 loss: 0.4176171206304571\n",
      "LOSS train 0.4176171206304571 valid 0.43665653467178345\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.4154847133978619\n",
      "  batch 2000 loss: 0.40949514190189074\n",
      "  batch 3000 loss: 0.38998655581107594\n",
      "  batch 4000 loss: 0.3735283136011567\n",
      "  batch 5000 loss: 0.3720889687231902\n",
      "  batch 6000 loss: 0.36403142652637327\n",
      "  batch 7000 loss: 0.3615584609006182\n",
      "  batch 8000 loss: 0.3846630826703622\n",
      "  batch 9000 loss: 0.38128196386038327\n",
      "  batch 10000 loss: 0.34631388822999\n",
      "  batch 11000 loss: 0.35364391168567816\n",
      "  batch 12000 loss: 0.3524102678061754\n",
      "  batch 13000 loss: 0.36534412119706394\n",
      "  batch 14000 loss: 0.36429404064803383\n",
      "  batch 15000 loss: 0.3360253882840625\n",
      "LOSS train 0.3360253882840625 valid 0.3846898078918457\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.3456082121549698\n",
      "  batch 2000 loss: 0.3293185201612505\n",
      "  batch 3000 loss: 0.3305832968052873\n",
      "  batch 4000 loss: 0.3363874676524138\n",
      "  batch 5000 loss: 0.3269679344208125\n",
      "  batch 6000 loss: 0.32457517654092227\n",
      "  batch 7000 loss: 0.3108927250001288\n",
      "  batch 8000 loss: 0.3210604087753018\n",
      "  batch 9000 loss: 0.30876280927357946\n",
      "  batch 10000 loss: 0.3088078941761196\n",
      "  batch 11000 loss: 0.33076372179668395\n",
      "  batch 12000 loss: 0.3378225366655097\n",
      "  batch 13000 loss: 0.3317111492676049\n",
      "  batch 14000 loss: 0.2901937160085363\n",
      "  batch 15000 loss: 0.31433694747301705\n",
      "LOSS train 0.31433694747301705 valid 0.34745416045188904\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.29028238151039115\n",
      "  batch 2000 loss: 0.30521470631732883\n",
      "  batch 3000 loss: 0.2990595210894098\n",
      "  batch 4000 loss: 0.28539402223634536\n",
      "  batch 5000 loss: 0.2921490096542257\n",
      "  batch 6000 loss: 0.2950614935187332\n",
      "  batch 7000 loss: 0.2994355448297429\n",
      "  batch 8000 loss: 0.2946384008266323\n",
      "  batch 9000 loss: 0.29303137222779335\n",
      "  batch 10000 loss: 0.28972769282019273\n",
      "  batch 11000 loss: 0.3045091270313578\n",
      "  batch 12000 loss: 0.2928738651769527\n",
      "  batch 13000 loss: 0.2874571303728735\n",
      "  batch 14000 loss: 0.30227124578044823\n",
      "  batch 15000 loss: 0.29325565133219245\n",
      "LOSS train 0.29325565133219245 valid 0.317562073469162\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.27433402013713204\n",
      "  batch 2000 loss: 0.27128889432655706\n",
      "  batch 3000 loss: 0.2653714262925387\n",
      "  batch 4000 loss: 0.28704491542804683\n",
      "  batch 5000 loss: 0.2801762724506334\n",
      "  batch 6000 loss: 0.27521771831773\n",
      "  batch 7000 loss: 0.2736721928370243\n",
      "  batch 8000 loss: 0.27569686727694714\n",
      "  batch 9000 loss: 0.28339211158431865\n",
      "  batch 10000 loss: 0.28059001547563095\n",
      "  batch 11000 loss: 0.2958661934709198\n",
      "  batch 12000 loss: 0.2720466659432568\n",
      "  batch 13000 loss: 0.28572548550638294\n",
      "  batch 14000 loss: 0.25697879608185215\n",
      "  batch 15000 loss: 0.2795506606234976\n",
      "LOSS train 0.2795506606234976 valid 0.31557807326316833\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a saved version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = GarmentClassifier()\n",
    "saved_model.load_state_dict(torch.load(\"model_20240922_083248_0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve loaded the model, it’s ready for whatever you need it for - more training, inference, or analysis.\n",
    "\n",
    "Note that if your model has constructor parameters that affect model structure, you’ll need to provide them and configure the model identically to the state in which it was saved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Understanding with Captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A First Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let’s take a simple, visual example. We’ll start with a ResNet model pretrained on the ImageNet dataset. We’ll get a test input, and use different Feature Attribution algorithms to examine how the input images affect the output, and see a helpful visualization of this input attribution map for some test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, some imports:\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll use the TorchVision model library to download a pretrained ResNet. Since we’re not training, we’ll place it in evaluation mode for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The place where you got this interactive notebook should also have an img folder with a file cat.jpg in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/anjalisuman/Documents/GitHub/ML101/img/cat.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg/cat.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m test_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/correct/path/to/cat.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#test_img = Image.open('/Users/anjalisuman/Documents/GitHub/ML101/img/cat.jpg')\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/PIL/Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/anjalisuman/Documents/GitHub/ML101/img/cat.jpg'"
     ]
    }
   ],
   "source": [
    "test_img = Image.open('img/cat.jpg')\n",
    "test_img = Image.open('/correct/path/to/cat.jpg')\n",
    "#test_img = Image.open('/Users/anjalisuman/Documents/GitHub/ML101/img/cat.jpg')\n",
    "\n",
    "test_img_data = np.asarray(test_img)\n",
    "plt.imshow(test_img_data)\n",
    "plt.show()\n",
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
