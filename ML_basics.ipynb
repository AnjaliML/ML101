{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "  root =\"data\",\n",
    "  train = True,\n",
    "  download = True,\n",
    "  transform = ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets,\n",
    "test_data = datasets.FashionMNIST(\n",
    "  root=\"data\",\n",
    "  train = False,\n",
    "  download = True,\n",
    "  transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "shape of y: torch.Size([64]) <built-in method type of Tensor object at 0x17d15cb40>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "  print(f\"shape of X [N, C, H, W]: {X.shape}\")\n",
    "  print(f\"shape of y: {y.shape} {y.type}\")\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "  \"cuda\"\n",
    "  if torch.cuda.is_available()\n",
    "  else \"mps\"\n",
    "  if torch.backends.mps.is_available()\n",
    "  else \"cpu\"\n",
    "\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "      nn.Linear(28*28, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512,10)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.flatten(x)\n",
    "    logits = self.linear_relu_stack(x)\n",
    "    return logits\n",
    "  \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303643  [   64/60000]\n",
      "loss: 2.293945  [ 6464/60000]\n",
      "loss: 2.280457  [12864/60000]\n",
      "loss: 2.265203  [19264/60000]\n",
      "loss: 2.255441  [25664/60000]\n",
      "loss: 2.229511  [32064/60000]\n",
      "loss: 2.227628  [38464/60000]\n",
      "loss: 2.206151  [44864/60000]\n",
      "loss: 2.196387  [51264/60000]\n",
      "loss: 2.154268  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.6%, Avg loss: 2.166437 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.180491  [   64/60000]\n",
      "loss: 2.173079  [ 6464/60000]\n",
      "loss: 2.123580  [12864/60000]\n",
      "loss: 2.122919  [19264/60000]\n",
      "loss: 2.086372  [25664/60000]\n",
      "loss: 2.026135  [32064/60000]\n",
      "loss: 2.044775  [38464/60000]\n",
      "loss: 1.983426  [44864/60000]\n",
      "loss: 1.977143  [51264/60000]\n",
      "loss: 1.893721  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 1.911402 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.950596  [   64/60000]\n",
      "loss: 1.922688  [ 6464/60000]\n",
      "loss: 1.814858  [12864/60000]\n",
      "loss: 1.832815  [19264/60000]\n",
      "loss: 1.740941  [25664/60000]\n",
      "loss: 1.685168  [32064/60000]\n",
      "loss: 1.696065  [38464/60000]\n",
      "loss: 1.611810  [44864/60000]\n",
      "loss: 1.627666  [51264/60000]\n",
      "loss: 1.507161  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 1.540598 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.616785  [   64/60000]\n",
      "loss: 1.576048  [ 6464/60000]\n",
      "loss: 1.429451  [12864/60000]\n",
      "loss: 1.484566  [19264/60000]\n",
      "loss: 1.376922  [25664/60000]\n",
      "loss: 1.367919  [32064/60000]\n",
      "loss: 1.377817  [38464/60000]\n",
      "loss: 1.307929  [44864/60000]\n",
      "loss: 1.336349  [51264/60000]\n",
      "loss: 1.229332  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.264664 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.351059  [   64/60000]\n",
      "loss: 1.326210  [ 6464/60000]\n",
      "loss: 1.160708  [12864/60000]\n",
      "loss: 1.255762  [19264/60000]\n",
      "loss: 1.139809  [25664/60000]\n",
      "loss: 1.164123  [32064/60000]\n",
      "loss: 1.185809  [38464/60000]\n",
      "loss: 1.121732  [44864/60000]\n",
      "loss: 1.152661  [51264/60000]\n",
      "loss: 1.068399  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.095880 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors: what are these and how to use them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly from data\n",
    "data = [[1,2], [3,4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a Numpy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.3113, 0.7818],\n",
      "        [0.1021, 0.4849]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From another tensor:\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.6078, 0.2804, 0.2838],\n",
      "        [0.6219, 0.1621, 0.0458]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#With random or constant values:\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Tensor Attributes\n",
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Operations\n",
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Standard numpy-like indexing and slicing:\n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To join tensors, we can use torch.cat to concatenate a sequence of tensors along a specified dimension. Another tensor joining operation, torch.stack, is similar but subtly different from torch.cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Multiplying Tensors\n",
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This computes the matrix multiplication between two tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **In-place operations** Operations that have a _ suffix are in-place. For example: x.copy_(Y), x.t_(), will change x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating and Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Dataset for your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __ init __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __init__ is a special function used to set up newly created objects. It's part of a class definition and runs automatically when creating a new instance of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The labels.csv file looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tshirt1.jpg, 0\n",
    "# tshirt2.jpg, 0\n",
    "\n",
    "# ankleboot999.jpg, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __ len __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __ len __ is a special method that defines how the len() function behaves for class instances. Implementing __ len __ in your class enables objects to be passed to len(), returning an integer representing the object's size or number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "def __len__(self):\n",
    "    return len(self.img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __ getitem __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __getitem__ method in Python is a special function that defines how elements are accessed using square brackets ([]). Implementing __getitem__ in your class enables instances to support indexing and slicing, similar to built-in data structures like lists, tuples, and dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    image = read_image(img_path)\n",
    "    label = self.img_labels.iloc[idx, 1]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing your data for training with DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data often requires preprocessing before it's suitable for training machine learning algorithms. Transforms help manipulate the data into the appropriate format.\n",
    "\n",
    "TorchVision datasets include two parameters: 'transform' for modifying features and 'target_transform' for altering labels. These accept callables containing transformation logic. The torchvision.transforms module provides many ready-to-use transforms.\n",
    "\n",
    "FashionMNIST features are in PIL Image format with integer labels. For training, we need normalized tensor features and one-hot encoded tensor labels. We use ToTensor and Lambda transforms to achieve these conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToTensor converts a PIL image or NumPy ndarray into a FloatTensor. and scales the image’s pixel intensity values in the range [0., 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, Lambda transforms apply custom, user-defined transformations to data within the torchvision.transforms module. This feature offers flexibility for preprocessing steps not covered by torchvision's standard transformations. The Lambda transform takes a function as an argument and applies it to each dataset element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A  neural network consists of interconnected nodes (neurons) that process and learn from data through weighted connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `torch.nn` is a module in PyTorch that provides essential tools for building and training neural networks. It includes pre-defined layers, loss functions, and utilities to streamline the creation, customization, and optimization of deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Device for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([8])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let’s break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Flatten, nn.Linear, nn.ReLU, nn.Sequential, nn.Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Flatten converts multi-dimensional input tensors into 1-dimensional tensors in neural networks, particularly in PyTorch.\n",
    "\n",
    "nn.Linear, also called a fully connected or dense layer, is a key component in neural networks. It performs linear transformations on input data, mapping it to an output space.\n",
    "\n",
    "nn.ReLU (Rectified Linear Unit) is a popular activation function that introduces non-linearity into neural networks, essential for learning complex patterns.\n",
    "\n",
    "nn.Sequential is a PyTorch container module that allows stacking layers in order, useful for building simple feed-forward neural networks where each layer's output becomes the next layer's input.\n",
    "\n",
    "nn.Softmax is an activation function used in the final layer of classification models. It transforms raw output scores (logits) into probabilities, making them easier to interpret and use for decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n",
      "torch.Size([3, 20])\n",
      "Before ReLU:  tensor([[-0.3264,  0.2045, -0.2844, -0.4914, -0.1212,  0.0925, -0.5322, -0.4351,\n",
      "          0.0932,  0.3811, -0.0162,  0.0242, -0.5544,  0.7543,  0.3187, -0.1645,\n",
      "          0.5500,  0.6467,  0.1858,  0.0836],\n",
      "        [-0.0144,  0.4569, -0.6450, -0.4188, -0.4306,  0.4422, -0.4905, -0.3342,\n",
      "          0.3966,  0.1537,  0.3626,  0.3200, -0.3123,  0.6153,  0.3389, -0.3378,\n",
      "          0.5751,  0.7267,  0.5071,  0.3118],\n",
      "        [-0.1139,  0.3702, -0.2975, -0.4764,  0.0751,  0.3665, -0.4987, -0.4759,\n",
      "          0.2069,  0.3965, -0.2714,  0.1578, -0.3417,  0.4921,  0.0848, -0.3314,\n",
      "          0.5995,  0.5459,  0.1521,  0.1647]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.2045, 0.0000, 0.0000, 0.0000, 0.0925, 0.0000, 0.0000, 0.0932,\n",
      "         0.3811, 0.0000, 0.0242, 0.0000, 0.7543, 0.3187, 0.0000, 0.5500, 0.6467,\n",
      "         0.1858, 0.0836],\n",
      "        [0.0000, 0.4569, 0.0000, 0.0000, 0.0000, 0.4422, 0.0000, 0.0000, 0.3966,\n",
      "         0.1537, 0.3626, 0.3200, 0.0000, 0.6153, 0.3389, 0.0000, 0.5751, 0.7267,\n",
      "         0.5071, 0.3118],\n",
      "        [0.0000, 0.3702, 0.0000, 0.0000, 0.0751, 0.3665, 0.0000, 0.0000, 0.2069,\n",
      "         0.3965, 0.0000, 0.1578, 0.0000, 0.4921, 0.0848, 0.0000, 0.5995, 0.5459,\n",
      "         0.1521, 0.1647]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "flatten =  nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "#nn.Linear \n",
    "layer1 = nn.Linear (in_features=28*28, out_features = 20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n",
    "#nn.ReLU \n",
    "print(f\"Before ReLU:  {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n",
    "#nn.Sequential \n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "#nn.Softmax\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0222,  0.0123, -0.0129,  ..., -0.0325, -0.0223, -0.0179],\n",
      "        [-0.0140,  0.0023,  0.0340,  ..., -0.0356,  0.0184, -0.0194]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0051,  0.0109], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0049,  0.0060,  0.0289,  ..., -0.0018,  0.0399, -0.0032],\n",
      "        [ 0.0007,  0.0043,  0.0098,  ..., -0.0324, -0.0035,  0.0187]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0306, -0.0433], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0255, -0.0216, -0.0087,  ..., -0.0235,  0.0323,  0.0042],\n",
      "        [ 0.0244, -0.0316,  0.0187,  ..., -0.0285, -0.0291, -0.0022]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0088, -0.0090], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "  print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation with torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad= True)\n",
    "b = torch.randn(3, requires_grad = True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x308fff850>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x309329d50>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1705, 0.2581, 0.0210],\n",
      "        [0.1705, 0.2581, 0.0210],\n",
      "        [0.1705, 0.2581, 0.0210],\n",
      "        [0.1705, 0.2581, 0.0210],\n",
      "        [0.1705, 0.2581, 0.0210]])\n",
      "tensor([0.1705, 0.2581, 0.0210])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "\n",
    " We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n",
    "\n",
    "We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling Gradient Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, all tensors with requires_grad=True track their computational history and support gradient computation. However, in some cases, such as when applying a trained model to input data, we only need forward computations through the network. To stop tracking computations, we can wrap our code in a torch.no_grad() block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "  z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Another way to achieve the same result is to use the detach() method on the tensor:\n",
    "z = torch.matmul(x,w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Reading: Tensor Gradients and Jacobian Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp+1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisite Code\n",
    "# we load the code from the previous sections on Datasets and Data Loaders and Build Model.\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Implementation\n",
    "We define train_loop that loops over our optimization code, and test_loop that evaluates the model’s performance against our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the loss function and optimizer, and pass it to train_loop and test_loop. Feel free to increase the number of epochs to track the model’s improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.301539  [   64/60000]\n",
      "loss: 2.289985  [ 6464/60000]\n",
      "loss: 2.278532  [12864/60000]\n",
      "loss: 2.271997  [19264/60000]\n",
      "loss: 2.235570  [25664/60000]\n",
      "loss: 2.215268  [32064/60000]\n",
      "loss: 2.213233  [38464/60000]\n",
      "loss: 2.177849  [44864/60000]\n",
      "loss: 2.176032  [51264/60000]\n",
      "loss: 2.145626  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 50.6%, Avg loss: 2.138972 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.147517  [   64/60000]\n",
      "loss: 2.137679  [ 6464/60000]\n",
      "loss: 2.080538  [12864/60000]\n",
      "loss: 2.096019  [19264/60000]\n",
      "loss: 2.030731  [25664/60000]\n",
      "loss: 1.976007  [32064/60000]\n",
      "loss: 1.994684  [38464/60000]\n",
      "loss: 1.913609  [44864/60000]\n",
      "loss: 1.914102  [51264/60000]\n",
      "loss: 1.844186  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.842278 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.878593  [   64/60000]\n",
      "loss: 1.849953  [ 6464/60000]\n",
      "loss: 1.727505  [12864/60000]\n",
      "loss: 1.766215  [19264/60000]\n",
      "loss: 1.652912  [25664/60000]\n",
      "loss: 1.612660  [32064/60000]\n",
      "loss: 1.625963  [38464/60000]\n",
      "loss: 1.536497  [44864/60000]\n",
      "loss: 1.555703  [51264/60000]\n",
      "loss: 1.455592  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 1.475139 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.545245  [   64/60000]\n",
      "loss: 1.516777  [ 6464/60000]\n",
      "loss: 1.364835  [12864/60000]\n",
      "loss: 1.434043  [19264/60000]\n",
      "loss: 1.319282  [25664/60000]\n",
      "loss: 1.316594  [32064/60000]\n",
      "loss: 1.327877  [38464/60000]\n",
      "loss: 1.262088  [44864/60000]\n",
      "loss: 1.291189  [51264/60000]\n",
      "loss: 1.197827  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.224305 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.301986  [   64/60000]\n",
      "loss: 1.291221  [ 6464/60000]\n",
      "loss: 1.122806  [12864/60000]\n",
      "loss: 1.225674  [19264/60000]\n",
      "loss: 1.109686  [25664/60000]\n",
      "loss: 1.126829  [32064/60000]\n",
      "loss: 1.149748  [38464/60000]\n",
      "loss: 1.094231  [44864/60000]\n",
      "loss: 1.128028  [51264/60000]\n",
      "loss: 1.048643  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 1.070209 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.140354  [   64/60000]\n",
      "loss: 1.151094  [ 6464/60000]\n",
      "loss: 0.963671  [12864/60000]\n",
      "loss: 1.096610  [19264/60000]\n",
      "loss: 0.982201  [25664/60000]\n",
      "loss: 0.999390  [32064/60000]\n",
      "loss: 1.039913  [38464/60000]\n",
      "loss: 0.988287  [44864/60000]\n",
      "loss: 1.020888  [51264/60000]\n",
      "loss: 0.954738  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.970432 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.027372  [   64/60000]\n",
      "loss: 1.059488  [ 6464/60000]\n",
      "loss: 0.854092  [12864/60000]\n",
      "loss: 1.010748  [19264/60000]\n",
      "loss: 0.901941  [25664/60000]\n",
      "loss: 0.909101  [32064/60000]\n",
      "loss: 0.967667  [38464/60000]\n",
      "loss: 0.919046  [44864/60000]\n",
      "loss: 0.946278  [51264/60000]\n",
      "loss: 0.890569  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.901817 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.943448  [   64/60000]\n",
      "loss: 0.994927  [ 6464/60000]\n",
      "loss: 0.775093  [12864/60000]\n",
      "loss: 0.949722  [19264/60000]\n",
      "loss: 0.848091  [25664/60000]\n",
      "loss: 0.842585  [32064/60000]\n",
      "loss: 0.916216  [38464/60000]\n",
      "loss: 0.872080  [44864/60000]\n",
      "loss: 0.892471  [51264/60000]\n",
      "loss: 0.843383  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.851999 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.878337  [   64/60000]\n",
      "loss: 0.945810  [ 6464/60000]\n",
      "loss: 0.715806  [12864/60000]\n",
      "loss: 0.903796  [19264/60000]\n",
      "loss: 0.809529  [25664/60000]\n",
      "loss: 0.792748  [32064/60000]\n",
      "loss: 0.876678  [38464/60000]\n",
      "loss: 0.838570  [44864/60000]\n",
      "loss: 0.851909  [51264/60000]\n",
      "loss: 0.806655  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.813881 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.825988  [   64/60000]\n",
      "loss: 0.905905  [ 6464/60000]\n",
      "loss: 0.669520  [12864/60000]\n",
      "loss: 0.867769  [19264/60000]\n",
      "loss: 0.779993  [25664/60000]\n",
      "loss: 0.754563  [32064/60000]\n",
      "loss: 0.844181  [38464/60000]\n",
      "loss: 0.813392  [44864/60000]\n",
      "loss: 0.820240  [51264/60000]\n",
      "loss: 0.776732  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.783279 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load the Model\n",
    "In this section we will look at how to persist model state with saving, loading and running model predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Model Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models store the learned parameters in an internal state dictionary, called state_dict. These can be persisted via the torch.save method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/anjalisuman/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:06<00:00, 85.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(weights='IMAGENET1K_V1')\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load model weights, you need to create an instance of the same model first, and then load the parameters using load_state_dict() method.\n",
    "\n",
    "In the code below, we set weights_only=True to limit the functions executed during unpickling to only those necessary for loading weights. Using weights_only=True is considered a best practice when loading weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model\n",
    "model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models with Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading model weights, we needed to instantiate the model class first, because the class defines the structure of a network. We might want to save the structure of this class together with the model, in which case we can pass model (and not model.state_dict()) to the saving function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load the model as demonstrated below.\n",
    "\n",
    "As described in Saving and loading torch.nn.Modules, saving state_dict``s is considered the best practice. However, below we use ``weights_only=False because this involves loading the model, which is a legacy use case for torch.save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pth', weights_only=False),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "This approach uses Python pickle module when serializing the model, thus it relies on the actual class definition to be available when loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
