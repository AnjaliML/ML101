{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "  root =\"data\",\n",
    "  train = True,\n",
    "  download = True,\n",
    "  transform = ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets,\n",
    "test_data = datasets.FashionMNIST(\n",
    "  root=\"data\",\n",
    "  train = False,\n",
    "  download = True,\n",
    "  transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Function to display a batch of images\n",
    "def show_images(images, labels, num_images=4):\n",
    "    # Unnormalize the images if necessary (assuming they were normalized)\n",
    "    # images = images * std + mean  # Uncomment and adjust if normalization was applied\n",
    "\n",
    "    # Convert the batch of images to a grid\n",
    "    img_grid = torchvision.utils.make_grid(images[:num_images], nrow=num_images)\n",
    "    \n",
    "    # Convert the grid to a numpy array and transpose the dimensions\n",
    "    img_grid = img_grid.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Plot the grid\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(img_grid)\n",
    "    plt.title('Sample Images from Test DataLoader')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "shape of y: torch.Size([64]) <built-in method type of Tensor object at 0x31e78b7a0>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAADzCAYAAAAB42CFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAucElEQVR4nO3deXhV1b3/8U8k80BIAiEBahiFqCFAUWQQUFQsxQnRFkes1gF7vVqtEfWKA1Whj/rUWy0OrTi0Ra2IikK9IHDxCgpFQKoIogxBQCBIJhKSsH5/+CP1mPXd5sSNQHy/nsc/8tlnr73OOXufk+U5+RDjnHMCAAAAgBAdcbAnAAAAAKD5YaEBAAAAIHQsNAAAAACEjoUGAAAAgNCx0AAAAAAQOhYaAAAAAELHQgMAAABA6FhoAAAAAAgdCw0AAAAAoWOhATQT7777rs455xwdeeSRSkhIUNu2bdW/f3/deOONB3tq32rs2LHq2LFjaONNnTpVMTExWrp0aWhjHq7mzp2rvn37KiUlRTExMZoxY8bBnpJXx44dFRMT863/TZ06NZTj3XvvvVE9Fl+fQ4sWLZSRkaHCwkJdddVVWrx48fc6F2t+v/rVr77TGGEZOnSohg4derCnAeAQEHuwJwDgu3v99dd15plnaujQoZo8ebJyc3O1ZcsWLV26VNOmTdMDDzxwsKeIg8A5p/PPP19HHXWUXn31VaWkpKh79+4He1peL7/8sqqrq+t/fvLJJ/WnP/1Js2fPVnp6en3epUuXUI537733avTo0Tr77LMbvc/o0aN14403yjmn0tJSrVq1Ss8884wef/xxXXfddfr973//vc0FAA4HLDSAZmDy5Mnq1KmT/vGPfyg29t+X9c9//nNNnjz5IM4MB9Pnn3+ukpISnXPOORo2bFjgbSsrK5WcnPw9zayh3r17R/w8e/ZsSdKPf/xjtW7d+mBMqYG2bdvqhBNOqP95+PDhuv7663XllVfq4YcfVo8ePXTNNdccxBk2P845VVVVKSkp6WBPBUAT8NUpoBnYuXOnWrduHbHI2O+IIyIv8+eff16nnXaacnNzlZSUpPz8fN1yyy2qqKiIuN3YsWOVmpqq1atXa/jw4UpJSVFubq7uv/9+SdLixYs1aNAgpaSk6KijjtLTTz8dsf/+ry/9z//8jy677DJlZmYqJSVFZ5xxhj799NNvvU/OOT366KPq1auXkpKSlJGRodGjRzdqX5/ven+2b9+ucePG6eijj1Zqaqqys7N18skna+HChQ2OVVxcrNGjRystLU2tWrXShRdeqCVLlni/+rN06VKdeeaZyszMVGJionr37q0XXngh4jaVlZW66aab1KlTJyUmJiozM1N9+/bV3/72N/P+3nnnnerQoYMkqaioSDExMfVfT7vzzjsVExOjZcuWafTo0crIyKj/pKCqqkrjx49Xp06dFB8fr/bt2+vaa6/Vl19+GTF+x44dNXLkSM2cOVO9e/euP5dmzpwp6avnPz8/XykpKTr++OND+RpbY8+J999/XyNHjlR2drYSEhLUrl07/fSnP1VxcbGkr75mVFFRoaeffrr+61BN/apPixYt9Ic//EGtW7fW7373u/q8qqpKN954o3r16qX09HRlZmaqf//+euWVVyL2D5pLNOdcY5SUlGjcuHFq37694uPj1blzZ912220RnyRJ0iOPPKLBgwcrOztbKSkpKigo0OTJk1VTUxNxO+ecJk+erLy8PCUmJqpPnz6aNWuW99ilpaX15/D+8+r6669v8Lqz/ytgU6ZMUX5+vhISEhpciwAOH3yiATQD/fv315NPPqnrrrtOF154ofr06aO4uDjvbdeuXasRI0bo+uuvV0pKilavXq1Jkybpvffe01tvvRVx25qaGo0aNUpXX321fvOb3+ivf/2rxo8fr9LSUr300ksqKipShw4d9N///d8aO3asjj32WP34xz+OGOPyyy/Xqaeeqr/+9a/atGmTbr/9dg0dOlQrV65Uq1atzPt01VVXaerUqbruuus0adIklZSU6O6779aAAQO0YsUKtW3bNurH6bvcn5KSEknShAkTlJOTo/Lycr388ssaOnSo5s6dW//LYUVFhU466SSVlJRo0qRJ6tq1q2bPnq2f/exnDeYzb948nX766erXr5+mTJmi9PR0TZs2TT/72c9UWVmpsWPHSpJ+/etf69lnn9XEiRPVu3dvVVRUaNWqVdq5c6d5X6+44goVFhZq1KhR+o//+A9dcMEFSkhIiLjNqFGj9POf/1xXX321Kioq5JzT2Wefrblz52r8+PE68cQTtXLlSk2YMEGLFi3SokWLIsZYsWKFxo8fr9tuu03p6em66667NGrUKI0fP15z587Vvffeq5iYGBUVFWnkyJH67LPPvtP/mW7MOVFRUaFTTz1VnTp10iOPPKK2bdtq69atmjdvnsrKyiRJixYt0sknn6yTTjpJ//Vf/yVJatmyZZPnlZSUpFNOOUXTpk1TcXGxOnTooOrqapWUlOimm25S+/bttXfvXs2ZM0ejRo3SU089pUsuueRb59LYc64xqqqqdNJJJ2ndunW666671LNnTy1cuFD33Xefli9frtdff73+tuvWrdMFF1xQvyhYsWKFfvvb32r16tX685//XH+7u+66S3fddZcuv/xyjR49Wps2bdIvf/lL1dXVRXxFr7KyUkOGDFFxcbFuvfVW9ezZU//61790xx136IMPPtCcOXMUExNTf/sZM2Zo4cKFuuOOO5STk6Ps7OzonxQAhwYH4LC3Y8cON2jQICfJSXJxcXFuwIAB7r777nNlZWXmfvv27XM1NTVuwYIFTpJbsWJF/bZLL73USXIvvfRSfVZTU+PatGnjJLlly5bV5zt37nQtWrRwv/71r+uzp556ykly55xzTsQx/+///s9JchMnTow4Vl5eXv3PixYtcpLcAw88ELHvpk2bXFJSkrv55psDH4/9x16yZElo9+ebamtrXU1NjRs2bFjEfXzkkUecJDdr1qyI21911VVOknvqqafqsx49erjevXu7mpqaiNuOHDnS5ebmurq6Ouecc8cee6w7++yzA++zz2effeYkud/97ncR+YQJE5wkd8cdd0Tks2fPdpLc5MmTI/Lnn3/eSXKPP/54fZaXl+eSkpJccXFxfbZ8+XInyeXm5rqKior6fMaMGU6Se/XVVxs99/1z3L59u3Ou8efE0qVLnSQ3Y8aMwPFTUlLcpZde2uj5SHLXXnutub2oqMhJcu+++653+/7z5fLLL3e9e/du0lysc64x85syZYqT5F544YWIfNKkSU6Se/PNN7371dXVuZqaGvfMM8+4Fi1auJKSEuecc7t27XKJiYnm9T1kyJD67L777nNHHHFExPXonHN///vfnST3xhtvRNyP9PT0+uMAOLzx1SmgGcjKytLChQu1ZMkS3X///TrrrLO0Zs0ajR8/XgUFBdqxY0f9bT/99FNdcMEFysnJUYsWLRQXF6chQ4ZIkj766KOIcWNiYjRixIj6n2NjY9W1a1fl5uZGfKc+MzNT2dnZ2rBhQ4O5XXjhhRE/DxgwQHl5eZo3b555f2bOnKmYmBhddNFFqq2trf8vJydHhYWFmj9/flSPT1j3Z8qUKerTp48SExMVGxuruLg4zZ07N+JxW7BggdLS0nT66adH7DtmzJiInz/55BOtXr26/vH5+v0cMWKEtmzZoo8//liSdPzxx2vWrFm65ZZbNH/+fO3Zs6dJ9/+bzj333Iif93+itf+TlP3OO+88paSkaO7cuRF5r1691L59+/qf8/PzJX3VOvT1v/fYn/vOj8Zq7DnRtWtXZWRkqKioSFOmTNGHH37Y5GNGwznXIHvxxRc1cOBApaam1p8vf/rTnxpcZ0Eac841xltvvaWUlBSNHj06It//XH/9uX3//fd15plnKisrq/414pJLLlFdXZ3WrFkj6atPYqqqqszr++tmzpypY489Vr169Yp47oYPH66YmJgG1/PJJ5+sjIyMqO4fgEMTCw2gGenbt6+Kior04osv6vPPP9cNN9yg9evX1/9BeHl5uU488US9++67mjhxoubPn68lS5Zo+vTpktTgF9jk5GQlJiZGZPHx8crMzGxw7Pj4eFVVVTXIc3JyvFnQ1362bdsm55zatm2ruLi4iP8WL14csXCKxne5Pw8++KCuueYa9evXTy+99JIWL16sJUuW6PTTT4943Hbu3On9Wtc3s23btkmSbrrppgb3cdy4cZJUfz8ffvhhFRUVacaMGTrppJOUmZmps88+W2vXrm3S47Bfbm5uxM87d+5UbGys2rRpE5HHxMR4n7NvPm7x8fGBue/8aKzGnhPp6elasGCBevXqpVtvvVXHHHOM2rVrpwkTJjT4G4Mw7V9EtWvXTpI0ffp0nX/++Wrfvr2ee+45LVq0SEuWLNEvfvGLRj8OjT3nGmPnzp3KycmJ+IqSJGVnZys2Nrb+ud24caNOPPFEbd68Wb///e/r/wfGI488IunfrxH7b29d31+3bds2rVy5ssHzlpaWJudcg+v5m+clgMMXf6MBNFNxcXGaMGGCHnroIa1atUrSV/9X8/PPP9f8+fPrP8WQ1OAPfcO0detWb9a1a1dzn9atWysmJkYLFy5s8HcFkrzZgfbcc89p6NCh+uMf/xiR7//e/35ZWVl67733Guz/zcdhf5PS+PHjNWrUKO8x93/PPSUlpf778Nu2bav/dOOMM87Q6tWrm3yfvvlLZ1ZWlmpra7V9+/aIxYZzTlu3btVxxx3X5GN9V9GcEwUFBZo2bZqcc1q5cqWmTp2qu+++W0lJSbrllltCn9uePXs0Z84cdenSpf4P8J977jl16tRJzz//fMTj/M0/vA7S2HOuMbKysvTuu+/KORcxny+++EK1tbX15+OMGTNUUVGh6dOnR3wysXz58gbjSfb1/fV/F6d169ZKSkqK+PuOr/tmq9g3z0sAhy8+0QCagS1btnjz/V+v2P9/Wfe/gX/zF7XHHnvsgM3tL3/5S8TP77zzjjZs2BD4h6wjR46Uc06bN29W3759G/xXUFBwwOZriYmJafC4rVy5UosWLYrIhgwZorKysgbtO9OmTYv4uXv37urWrZtWrFjhvY99+/ZVWlpag3m0bdtWY8eO1ZgxY/Txxx+rsrIypHuo+grc5557LiJ/6aWXVFFR8a0VuQdSU86JmJgYFRYW6qGHHlKrVq20bNmy+m0JCQmhfAWtrq5Ov/rVr7Rz504VFRVFHDs+Pj7il+atW7c2aJ0Kmktjz7nGGDZsmMrLyxv8w4DPPPNM/fb9x9w/p/2cc3riiSci9jvhhBOUmJhoXt9fN3LkSK1bt05ZWVne5y7Mf6wTwKGFTzSAZmD48OHq0KGDzjjjDPXo0UP79u3T8uXL9cADDyg1NVX/+Z//Kemr709nZGTo6quv1oQJExQXF6e//OUvWrFixQGb29KlS3XFFVfovPPO06ZNm3Tbbbepffv29V8P8hk4cKCuvPJKXXbZZVq6dKkGDx6slJQUbdmyRW+//bYKCgq+93+vYOTIkbrnnns0YcIEDRkyRB9//LHuvvtuderUSbW1tfW3u/TSS/XQQw/poosu0sSJE9W1a1fNmjVL//jHPyRF1g0/9thj+slPfqLhw4dr7Nixat++vUpKSvTRRx9p2bJlevHFFyVJ/fr108iRI9WzZ09lZGToo48+0rPPPqv+/fuH+m9fnHrqqRo+fLiKiopUWlqqgQMH1rdO9e7dWxdffHFox4pWY8+JmTNn6tFHH9XZZ5+tzp07yzmn6dOn68svv9Spp55aP15BQYHmz5+v1157Tbm5uUpLS/vWf8xw27ZtWrx4sZxzKisrq/8H+1asWKEbbrhBv/zlL+tvO3LkSE2fPl3jxo2rb2S65557lJub2+Arb9ZcGnvO7bdu3Tr9/e9/b5AfffTRuuSSS/TII4/o0ksv1fr161VQUKC3335b9957r0aMGKFTTjlF0lfnQHx8vMaMGaObb75ZVVVV+uMf/6hdu3ZFjJmRkaGbbrpJEydOjLi+77zzzgZfnbr++uv10ksvafDgwbrhhhvUs2dP7du3Txs3btSbb76pG2+8Uf369Qt87AEcpg7O36ADCNPzzz/vLrjgAtetWzeXmprq4uLi3JFHHukuvvhi9+GHH0bc9p133nH9+/d3ycnJrk2bNu6KK65wy5Yta9CIdOmll7qUlJQGxxoyZIg75phjGuR5eXnupz/9af3P+5uf3nzzTXfxxRe7Vq1auaSkJDdixAi3du3aiH2/2Tq135///GfXr18/l5KS4pKSklyXLl3cJZdc4pYuXRr4eFitU9/l/lRXV7ubbrrJtW/f3iUmJro+ffq4GTNmeOe+ceNGN2rUKJeamurS0tLcueee69544w0nyb3yyisRt12xYoU7//zzXXZ2touLi3M5OTnu5JNPdlOmTKm/zS233OL69u3rMjIyXEJCguvcubO74YYb3I4dOwIfh29rndrf6PR1e/bscUVFRS4vL8/FxcW53Nxcd80117hdu3YFPj77ydN+ZM0jiDXHbzsnVq9e7caMGeO6dOnikpKSXHp6ujv++OPd1KlTI8ZZvny5GzhwoEtOTm7QkuSj/9/oJskdccQRrmXLlq6goMBdeeWVbtGiRd597r//ftexY0eXkJDg8vPz3RNPPFF/vxozl2jOua/P75v/TZgwwTn3VZva1Vdf7XJzc11sbKzLy8tz48ePd1VVVRFjvfbaa66wsNAlJia69u3bu9/85jdu1qxZTpKbN29e/e327dvn7rvvPvejH/3IxcfHu549e7rXXnvNDRkypMHjWV5e7m6//XbXvXt3Fx8f79LT011BQYG74YYb3NatWyPuR1B7FoDDS4xznqoMAPiOpk6dqssuu0xLlixR3759D/Z0Drp7771Xt99+uzZu3Fj/PX4AAJozvjoFACH7wx/+IEnq0aOHampq9NZbb+nhhx/WRRddxCIDAPCDwUIDAEKWnJyshx56SOvXr1d1dbWOPPJIFRUV6fbbbz/YUwMA4HvDV6cAAAAAhI56WwAAAAChY6EBAAAAIHQsNAAAAACEjoUGAAAAgNA1unUqJibmQM4DAAAAwGGiMX1SfKIBAAAAIHQsNAAAAACEjoUGAAAAgNCx0AAAAAAQOhYaAAAAAELHQgMAAABA6FhoAAAAAAgdCw0AAAAAoWOhAQAAACB0LDQAAAAAhI6FBgAAAIDQxR7sCQBoXtLS0rz58ccfb+4zd+7cAzWden369PHm5eXl5j5r1qw5UNPBIS4mJsabO+e8+bBhw8yxrrvuOm++fPlyb56Tk2OO9cknn3jz1NRUb56RkWGOVVNT4807d+5s7nPOOeeY29C8tWnTxptfeeWV3nz37t3mWHv27Inq2EFjWddkixYtvHl8fLw51hdffOHN58+f78337t1rjoWv8IkGAAAAgNCx0AAAAAAQOhYaAAAAAELHQgMAAABA6FhoAAAAAAhdjLP+XP+bNzQaOAAc/hITE7359ddf783HjBljjmW13FiNJZJUWVnpzTMzM819olVVVeXNg9pP6urqvPmCBQu8+ZNPPmmONXv27IDZ4VBzxBH+/w+3b98+b/7222+bYw0cODCUOUlSaWmpN09OTvbmsbF2uaR13VljSdIZZ5zhzWfOnGnug+Zh3Lhx3vzBBx/05iUlJeZYW7Zs8eZW49nmzZvNsax2wPz8fG9uvRdI0pw5c7z5ypUrvfmzzz5rjvVD0JglBJ9oAAAAAAgdCw0AAAAAoWOhAQAAACB0LDQAAAAAhI6FBgAAAIDQ2XUUAJqVSZMmmduuvPJKb56WlubNg5qarG1BDSRJSUnevKKiwptbjUCStHfvXm9uNewEjZWQkODNR44c6c3POussc6xFixZ588GDB5v74OCx2qUshYWF5jbr3N+xY4c3T0lJMcdq0aKFN9+5c6c3r62tNcey2iS7du1q7tOjRw9vTutU82c1B65fv96bW619Qaw2qqDX6aysLG/esmVLb241t0lSu3btvPnq1avNfRCMTzQAAAAAhI6FBgAAAIDQsdAAAAAAEDoWGgAAAABCx0IDAAAAQOhYaAAAAAAIHfW2QDNjVdXefPPN5j5bt2715la9rHPOHMuqzIyLizP3qaqqiioPOr61LTY2+pc76/jl5eXePKjOccCAAd78tdde8+ZnnHHGt8wOh5LU1FRzm1Vja9VvBlV5VldXe3Or9taqaA4aK8iPfvSjqPdB82DVyG7fvt2bd+7c2Rxr165d3ty6JsrKysyxWrVq5c2t9yKrtl2y3z8++OADcx8E4xMNAAAAAKFjoQEAAAAgdCw0AAAAAISOhQYAAACA0LHQAAAAABA6WqeAZuaee+7x5qWlpeY++/bt8+ZWk01OTk7U87JaRoKOX1tb681TUlLMsRITE715SUmJNw9q+LFapKwmH6vlRJK2bdvmzQcPHuzNW7dubY5ltRjhwGvbtm3U+9TU1Hhzq+Em6Jy0rknrWrGuraDjB71WZGdnm9vQvG3YsMGbFxYWevOgc896bbWaDvfu3WuOZV0TVptiZmamOZb1Gr569WpzHwTjEw0AAAAAoWOhAQAAACB0LDQAAAAAhI6FBgAAAIDQsdAAAAAAEDoWGgAAAABCR70t0Mykp6d78+rqanMfq07TqrF99NFHzbEef/xxb/7Pf/7T3MeqIezQoYM3LysrM8fauHGjN7dqOYNqE3Nzc7355s2bvXlVVZU5VsuWLb15UlKSN+/cubM5FvW2B8+xxx4b9T5Wva313FvVn0HbrGs4qHLZqgUNeq0Iql1G82bVIX/wwQfe3KqqlezzskuXLt48IyPDHMs699esWWPuY/n000+9uVUfjW/HJxoAAAAAQsdCAwAAAEDoWGgAAAAACB0LDQAAAAChY6EBAAAAIHS0TgHNTEJCgjcPakQKaqbxufXWW81tu3fv9uZWw41kt+/Mnz/fm5900kn25AwffvihN8/Pzzf3sZqirrvuOm8+ceJEc6zt27d7c6sxZdCgQeZY7733nrkNB1ZhYaE3D2ovs6695ORkb25dw5J9TpaUlHhzqylIsq/7oOMHNQmhedu3b58337Rpkze3XnMl+7wcPXq0N8/KyjLHOuaYY7z5ggULvHlQA6LVKBgfH+/NKysrzbHwFT7RAAAAABA6FhoAAAAAQsdCAwAAAEDoWGgAAAAACB0LDQAAAACho3UKTRbUImS1UwQ1oFisBpTq6mpzn27dunnztWvXRn38Q5XVgmGxnhMpuGXG55lnnjG3nXXWWVGNJUmZmZne3GqXuvvuu82xSktLvfmYMWOiOrYkHXnkkd78hRde8OZBrVNWu1RdXZ0379WrlzkWDp7jjjvOmwddX1a7VG1trTdPT083x1q2bJk3t86XXbt2mWNZr6HWfCW7YQjN30cffeTNhw0bFtXtJfvcs5qqgpr2HnvsMW9unavFxcXmWNb1smfPHnMfBOMTDQAAAAChY6EBAAAAIHQsNAAAAACEjoUGAAAAgNCx0AAAAAAQOhYaAAAAAEJHve1hICYmJuptVtVi+/btzbH69+/vzWfNmuXNKyoqzLHCFFRjaxk1apQ3nzRp0nedziGjXbt2Ud0+qH4zKSkpqrGCzqOmOO+886K6/bPPPmtus2oIrTrmFStWmGPl5uZ68/Ly8oDZhcOqaMbBlZ+f781ramrMfaxrLzU11Ztv2bLFHOuEE07w5lZ1uFWrHLQtNtb+1aCkpMTchubNqj22fhfIyckxxwqqXfYJOietenbr/K6qqjLHsiqnExMTvXlTfj/5oeETDQAAAAChY6EBAAAAIHQsNAAAAACEjoUGAAAAgNCx0AAAAAAQOlqnDnNBTUI+J554ormtX79+3txqN3r44YejOnZTZWdne/Phw4eb+5SVlR2o6Rwy2rRpE9pYcXFx3txq0glqnQpqubEsWLAgqtvPnj3b3Na5c2dvvnPnTm8+YsQIc6x58+Z5c6upKqiNynpcrJaToMYWHDzp6ene3Hoepehbp6ZPnx79xAxW25ok1dXVRT1efHz8d5kODmNWu5TVRhX0+4n1e4XVLrV8+XJzLKtxzWpTDDqHreslqFUOwfhEAwAAAEDoWGgAAAAACB0LDQAAAAChY6EBAAAAIHQsNAAAAACEjtapw0BQa4jVdHLcccd58/z8fHOsbdu2efNu3bp585dfftkcq6SkxJtbLRAbNmwwx8rKyvLmLVu2NPcpLi42tzUXQc1PPjExMVEfo7Ky0psHNSJZTSNBx+/evbs3v//++715ly5dzLEsH330kTfv0aOHuU9eXp43HzdunDfv37+/OZZ1Tezdu9ebR/v84vthteBZ14pkt+JY/va3v0V1e0mqrq725pmZmeY+VhNbEKthCM3fnj17vLn1mh/Uwmex9nn//fejHsv6fcO6H5J9HdE61XR8ogEAAAAgdCw0AAAAAISOhQYAAACA0LHQAAAAABA6FhoAAAAAQsdCAwAAAEDoqLc9hBxxhH/dZ1XYSlJKSoo3Hz16tDe3qtskKTEx0ZunpaV586C6Uuu+WPscc8wx5libNm3y5rt27TL3iY1t/qd2mzZtorq9VUEo2RXK1uMYVFv429/+1pvHxcWZ+5x22mnevLCw0Jsfe+yx5ljW+WrV2FoVupL0/PPPe/NevXqZ+1isx9h6XoIeLxw8Vr1r0DUR7evRvHnzorq9JC1atMibB1UuB1WnW5pSiYvmoa6uzptb1a9Btc7WtqZU4lZVVXnz+Ph4b15RUWGOZf2+Zd13fDs+0QAAAAAQOhYaAAAAAELHQgMAAABA6FhoAAAAAAgdCw0AAAAAoWv+1TweVvNRUEOC1aJk7RM0ltX00ZRWg6uvvtqbb9u2zZtb7QySlJeX582tNirrGFL0DTtBLRB79+715i1btjT3SUhI8OZWS1fQ8Q9Vubm5Ud0+qHXKOr+ttpzdu3ebY916661RzStoPOscO/roo6M+xtatW715UHtX0PXi05TrPuh5iXYsmlEOTVaDmNVwE9QOaFm/fr03HzRokLlPUHOgJejaR/O2Y8cOb2697lnvK5LdCBXta64klZWVeXPr/A46xueff+7Nm/I6ja/wiQYAAACA0LHQAAAAABA6FhoAAAAAQsdCAwAAAEDoWGgAAAAACB0LDQAAAAChO+zrbZtSVRu0zRJttZlVPylFX0E5ZswYc1tOTo43f//99725VVcqSa1atfLmO3fu9OYlJSXmWK1bt/bmaWlp3jzo8bIEVeclJyd7827dunnz5cuXR338gy2oljVaVoXw3LlzvfngwYPNsYqLi7150HlvVR1a56tVZxjEqhgNqmm2apKt4wdVf/bq1cubW9dXkI4dO3rzdevWRT0WwhH0vmKde2E+X9Z1F/Q62ZT3QvxwbdmyxZtbr99B9cnWe7R1rQSx3ies2vrS0lJzrKDrBU3DIwoAAAAgdCw0AAAAAISOhQYAAACA0LHQAAAAABA6FhoAAAAAQnfYt041pTXDahUIahuwGnOs40fbLCVJv/jFL7z5UUcdZe6zadMmb56VleXNg1ogkpKSvPnmzZu9udUgJdktXZWVld48MTHRHKspzWKW4cOHe/PDsXXKagmzpKammtusxpqnn37am48YMcIcy3qOg1jXnvXcB7WnWazzJWgsq3WqtrbWmz/11FPmWFbrVFNYrW60Th081jkhSSkpKd581apVoR3/jTfe8OY333yzuQ8NO4iG9dpu5eXl5eZY1rmXmZkZ9bysdinr9bu6utocqyktgAjGqwwAAACA0LHQAAAAABA6FhoAAAAAQsdCAwAAAEDoWGgAAAAACN0h1TrVlAYMq0kmqF3JakSy8qZo166duW3UqFHe3Gp9Wrt2rTmW1SRktS1YbVSStHfvXm9uPcbJycnmWBarjSuoBcLax2qakOzncuDAgQGzO7xY7RxNeb62b9/uzXft2hX1vGpqarx5ULtTUxrEomUdo0WLFlHvEx8f783ffffd0Oa1Z88ecx/agg49Qc+J9X702WefhXb8FStWeHPrXJWkuLi4qI8T9LqL5i3a9+Kga8J6P7Dei4JYvyNZv1MFnfdBDZhoGt6tAAAAAISOhQYAAACA0LHQAAAAABA6FhoAAAAAQsdCAwAAAEDoWGgAAAAACN0Brbe1aiOtirQw62WbUpfZpk0bc1vHjh29effu3b15bm6uOZZVI1taWurNW7VqZY7VsmVLb27Vt1m1t5L9+Ofl5UV1DEn68ssvvblVfRr03FsVeUH1n9a5V1ZW5s2POeYYc6x//etf5raDyTovrKrgoNq+8vJyb56fnx/1vGpra715U6o0w6y9tSpGg45hbbMe+6bM15pXUDVk69atoz4OwlFcXOzNg+qjrde3zz//PJQ5SfZ1FySo2tlCvS2+yaqqtSrYg/ZpSqX6hx9+6M07dOjgza3fm6Tg3yvQNHyiAQAAACB0LDQAAAAAhI6FBgAAAIDQsdAAAAAAEDoWGgAAAABCd0Bbp6x2KUvbtm3NbVbzUUpKSlS5JCUlJXnzTp06mftYjSJWi5LV4iPZbTLp6ene3JqvZDeNWPOtrKw0x7LaiuLj4735li1bzLGs+2LNK6hpIjU11ZtnZGSY+1jNKDk5Od48KyvLHOtQZTXGNKX56OOPP/bmXbp0iXos6/hBLUrWPlYjU1NYxwhq3rGuCev8/uKLL6Kel3X8oPse1JCHA2vbtm3ePOhasZ7jo446KpQ5SXabYZBo36Ol4HYt/DBZ759r16419/nJT37izR977LGoj79s2TJvfvzxx3vzzZs3m2MFvU+haXhEAQAAAISOhQYAAACA0LHQAAAAABA6FhoAAAAAQsdCAwAAAEDoDmjrlOWUU07x5u3atTP3sdqVrPaVoCYZq2nDOoYklZWVeXOrEclqN5LsNpmEhARvHtTIZDUkWPMKelyspiarQWv37t3mWNnZ2ea2aFn3f9++feY+VlOX9RgHPfeHqthY/+XblCaZNWvWePPBgwdHPZY1ryDWNWHlTWnWssYKahmJ9rwoLi6OeltTGs+s6xsH3pIlS7x5fn6+uY/VXlZYWBjKnJrKej0MYt0X/HANGTLEmwc1sVmtUxdffHHUx1+1apU3z8zM9ObXXnutOdbKlSu9+T//+c+o54Wv8IkGAAAAgNCx0AAAAAAQOhYaAAAAAELHQgMAAABA6FhoAAAAAAgdCw0AAAAAoTug9bannXaaN7/88su9+erVq82xtmzZ4s2t2tmgysq9e/d686DqV6sa0zp+fHy8OZZVy5qWlhbVsSW7xtU6RlxcnDmWVcnbtm1bb3700UebY1n3P+h5sVi1u8nJyeY+VVVV3tyq6v3iiy+intfBZt3HptTbWudLjx49vHlNTY05VlOe4zBZx7cqcYNqkqN9LLt27Wpu27p1qze3rjvrdUoKPvdxYP3v//6vN7/sssvMfazrpU+fPqHMKUjQORz0nmcJul7QvFm/i1jnUbdu3cyxPvnkE29uva8FsWrI09PTvXm/fv3MsYJ+R0LT8IkGAAAAgNCx0AAAAAAQOhYaAAAAAELHQgMAAABA6FhoAAAAAAjdAW2deu+997z5CSec4M0LCgrMsQYOHBjVsa0WAsluiiopKTH3sbbt3r3bmwe1TlnNDVlZWd68e/fu5lhW+0zLli29udW8I0mFhYXefOXKld58/fr15linnHKKN09ISIh6Xpag53jz5s3evLS01JtbjV+HMuv+N6VJxmrasM7JyspKc6ymHD9aTTlfLEEtOtHel7POOsvcZl0vvXv39uZB9zEjIyOqeSE877zzjjcPasuxrtXvo+3Oer+TghsNLQe7VQ4Hj/WaZP2+Y7ViSlJ1dXUocwo6fmys/1dcq40qaB80Ha8YAAAAAELHQgMAAABA6FhoAAAAAAgdCw0AAAAAoWOhAQAAACB0LDQAAAAAhO6A9nh9+eWX3vzuu++OeqzU1FRv3q9fP28eVAk7YMAAb96xY0dzn549e3rzlJQUbx5UG2hVxFk1m0G1ux988IE3nzNnjjd/4403zLGC6hmj9eqrr3rzI4880pvv2LHDHMuqZwyqbbTqJK1KvTVr1phjHarq6uq8eWJiYtRj9ejRw5tbtYFB1YRWPWBQjWy0NZtBt7e2NaUSN9p626DXEKsmevTo0d48aL5WHTEOvA0bNnhzqzpbsmu9rWu1c+fO5liffvppwOwaqqmpMbc1pcrz+6ivxuFl79693tyq2ZekioqK0I5vnePWe2TQ6+fWrVtDmRP+jU80AAAAAISOhQYAAACA0LHQAAAAABA6FhoAAAAAQsdCAwAAAEDoDmjrVJjKy8u9+dy5c6PKJenRRx8NZU6wnXnmmQd7Cs2e1fQRbYOTJGVkZHjzpKSkqI4tBbdLhbVPUCOTtc3Km9JgtXv3bm/ev39/c6xom82C7qP1vODgsZqlJLupydonzNapLVu2mNuslrSgpkNap/BNe/bs8eZBDYhhtlxG+154xBH2/2MPamlD0/CJBgAAAIDQsdAAAAAAEDoWGgAAAABCx0IDAAAAQOhYaAAAAAAI3WHTOgUgktWOYTWApKammmM9+OCD3nzYsGHePKj1qK6uztwWrWgbpKToW7eCWnSs+9KyZUtvPn/+fHOsmTNnevMJEyZEdWxJio+PN7chHNZ5ZJ17L7/8sjnWBRdcENUxBg0aZI41Z84cc5tPRUVFVLeXgq+hXbt2RT0emrecnBxvHvTaGtT8FC2rldRqMwyal/X+iabjEw0AAAAAoWOhAQAAACB0LDQAAAAAhI6FBgAAAIDQsdAAAAAAEDoWGgAAAABCR70tcJhKTk725lYtqlWHK0lxcXHefMeOHd68W7du5ljr1q3z5mHWGUZbYRu0j1WBKEm1tbXePDMz05t/8cUX5ljWY2kJqrfNy8uLaixEL9p621deecUc65JLLvHm1jV57rnnmmPdeeed5jaf2Fj7bb4p9dHV1dVRHR/N37Zt27x5dna2uY/12toUVuWy9RqakJBgjhX0Go6m4RMNAAAAAKFjoQEAAAAgdCw0AAAAAISOhQYAAACA0LHQAAAAABA6WqeAw9Q777zjzfv37+/Nq6qqzLHWrFnjzY866qjoJwavzp07e/OysjJvnpiYaI61ZMmSUOYEm9WSZrWUzZo1yxzLasWx2m+CmtCitWrVKnNbQUGBN9+zZ4+5T25u7neeE5oX69zv27evuU+Y57j1GlpaWurNg15bN2zYEMqc8G98ogEAAAAgdCw0AAAAAISOhQYAAACA0LHQAAAAABA6FhoAAAAAQkfrFHCYeu+997x5cnKyN9+7d685VpgNIPCLi4vz5lbzUHx8vDlWeXl5KHOCra6uLrSxNm7c6M1POOEEb56SkmKONWDAAG9utdC1aNHCHMtq37HOVUlq3bq1uQ0/TFajYVC7U5jXlyUpKcmbB11fxcXFB2o6P1h8ogEAAAAgdCw0AAAAAISOhQYAAACA0LHQAAAAABA6FhoAAAAAQsdCAwAAAEDoqLcFDlObN2/25suWLfPmVgWhJFVUVER17NhY+6XDqi2MiYmJ6hiHMuu+BFU2fvLJJ9789ddf9+bp6enmWIsXLw6YHcLgnAttrCeeeMKbr1692ptPmzbNHMuqsbU8++yz5jbrHAuqT164cGFUx0fz98wzz3jzQYMGmfvMmjXrQE2n3quvvhr1Ph988MEBmMkPG59oAAAAAAgdCw0AAAAAoWOhAQAAACB0LDQAAAAAhI6FBgAAAIDQxbhGVms0p8YYAAAAAE3XmCUEn2gAAAAACB0LDQAAAAChY6EBAAAAIHQsNAAAAACEjoUGAAAAgNCx0AAAAAAQOhYaAAAAAELHQgMAAABA6FhoAAAAAAgdCw0AAAAAoWOhAQAAACB0LDQAAAAAhC62sTd0zh3IeQAAAABoRvhEAwAAAEDoWGgAAAAACB0LDQAAAAChY6EBAAAAIHQsNAAAAACEjoUGAAAAgNCx0AAAAAAQOhYaAAAAAELHQgMAAABA6P4f5krIV/I8JZIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "  print(f\"shape of X [N, C, H, W]: {X.shape}\")\n",
    "  print(f\"shape of y: {y.shape} {y.type}\")\n",
    "  show_images(X, y, num_images=4)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "  \"cuda\"\n",
    "  if torch.cuda.is_available()\n",
    "  else \"mps\"\n",
    "  if torch.backends.mps.is_available()\n",
    "  else \"cpu\"\n",
    "\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "      nn.Linear(28*28, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512,10)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.flatten(x)\n",
    "    logits = self.linear_relu_stack(x)\n",
    "    return logits\n",
    "  \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.286817  [   64/60000]\n",
      "loss: 2.282568  [ 6464/60000]\n",
      "loss: 2.266541  [12864/60000]\n",
      "loss: 2.277120  [19264/60000]\n",
      "loss: 2.249187  [25664/60000]\n",
      "loss: 2.215700  [32064/60000]\n",
      "loss: 2.225198  [38464/60000]\n",
      "loss: 2.185065  [44864/60000]\n",
      "loss: 2.185690  [51264/60000]\n",
      "loss: 2.167311  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 39.1%, Avg loss: 2.154995 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.149534  [   64/60000]\n",
      "loss: 2.146959  [ 6464/60000]\n",
      "loss: 2.093741  [12864/60000]\n",
      "loss: 2.125121  [19264/60000]\n",
      "loss: 2.070948  [25664/60000]\n",
      "loss: 2.001034  [32064/60000]\n",
      "loss: 2.025791  [38464/60000]\n",
      "loss: 1.941438  [44864/60000]\n",
      "loss: 1.940474  [51264/60000]\n",
      "loss: 1.889186  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.880911 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.892779  [   64/60000]\n",
      "loss: 1.874226  [ 6464/60000]\n",
      "loss: 1.761399  [12864/60000]\n",
      "loss: 1.816728  [19264/60000]\n",
      "loss: 1.722449  [25664/60000]\n",
      "loss: 1.652759  [32064/60000]\n",
      "loss: 1.668473  [38464/60000]\n",
      "loss: 1.571326  [44864/60000]\n",
      "loss: 1.589790  [51264/60000]\n",
      "loss: 1.497699  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 1.516623 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.564496  [   64/60000]\n",
      "loss: 1.542357  [ 6464/60000]\n",
      "loss: 1.396436  [12864/60000]\n",
      "loss: 1.483264  [19264/60000]\n",
      "loss: 1.381174  [25664/60000]\n",
      "loss: 1.351985  [32064/60000]\n",
      "loss: 1.367905  [38464/60000]\n",
      "loss: 1.292659  [44864/60000]\n",
      "loss: 1.324756  [51264/60000]\n",
      "loss: 1.234951  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.259545 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.323019  [   64/60000]\n",
      "loss: 1.313886  [ 6464/60000]\n",
      "loss: 1.150180  [12864/60000]\n",
      "loss: 1.267401  [19264/60000]\n",
      "loss: 1.153374  [25664/60000]\n",
      "loss: 1.158801  [32064/60000]\n",
      "loss: 1.183988  [38464/60000]\n",
      "loss: 1.121310  [44864/60000]\n",
      "loss: 1.155899  [51264/60000]\n",
      "loss: 1.082037  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.098247 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "    # Visualize the image\n",
    "    plt.imshow(x.cpu().squeeze(), cmap='gray')\n",
    "    plt.title(f'Predicted: {predicted}, Actual: {actual}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating and Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Dataset for your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __ init __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __init__ is a special function used to set up newly created objects. It's part of a class definition and runs automatically when creating a new instance of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The labels.csv file looks like:\n",
    "\n",
    "- tshirt1.jpg, 0\n",
    "- tshirt2.jpg, 0\n",
    "- ankleboot999.jpg, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  __ len __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __ len __ is a special method that defines how the len() function behaves for class instances. Implementing __ len __ in your class enables objects to be passed to len(), returning an integer representing the object's size or number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "def __len__(self):\n",
    "    return len(self.img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __ getitem __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __getitem__ method in Python is a special function that defines how elements are accessed using square brackets ([]). Implementing __getitem__ in your class enables instances to support indexing and slicing, similar to built-in data structures like lists, tuples, and dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    image = read_image(img_path)\n",
    "    label = self.img_labels.iloc[idx, 1]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing your data for training with DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAd5klEQVR4nO3df2xV9f3H8dcFyrVoe2eD7b0V6BoHmQNCJjh+RAUNVruMiLgFNVnKP/6YQGRozBhZ7PYHdWYSlzBddAvDTBxZps5EptZBiwtjQ4KRoTE4q63QpqPBe0spLW0/3z8IzbcWsJ8P9973ve3zkZzE3nvenvc9/bQvTu+97xtxzjkBAGBgnHUDAICxixACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAmQnWDXzZwMCAjh07pqKiIkUiEet2AACenHPq7OxUeXm5xo27+LVOzoXQsWPHNHXqVOs2AACXqKWlRVOmTLnoPjn357iioiLrFgAAaTCS3+cZC6FnnnlGlZWVuuyyyzR37ly98847I6rjT3AAMDqM5Pd5RkJox44dWrdunTZu3KiDBw/qxhtvVHV1tZqbmzNxOABAnopkYor2/Pnzdd111+nZZ58dvO3aa6/V8uXLVVdXd9HaVCqlWCyW7pYAAFmWTCZVXFx80X3SfiXU29urAwcOqKqqasjtVVVV2rt377D9e3p6lEqlhmwAgLEh7SF0/Phx9ff3q6ysbMjtZWVlamtrG7Z/XV2dYrHY4MYr4wBg7MjYCxO+/ISUc+68T1Jt2LBByWRycGtpaclUSwCAHJP29wlNnjxZ48ePH3bV097ePuzqSJKi0aii0Wi62wAA5IG0XwlNnDhRc+fOVX19/ZDb6+vrtWjRonQfDgCQxzIyMWH9+vX64Q9/qHnz5mnhwoV67rnn1NzcrAcffDAThwMA5KmMhNDKlSvV0dGhX/ziF2ptbdWsWbO0c+dOVVRUZOJwAIA8lZH3CV0K3icEAKODyfuEAAAYKUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmJlg3cBYMX78eO+a/v7+DHQC5I6Kigrvms8++ywDncAKV0IAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMjOkBppFIJKjOOeddk61hpHfeead3zY033hh0rB//+MdBdb6ef/5575rnnnsu6FjvvvtuUF2umjVrVlDdD37wA++ahx9+2LsmFot51/znP//xrvnb3/7mXSNJu3fvztqxfGXz91cmcSUEADBDCAEAzKQ9hGpraxWJRIZs8Xg83YcBAIwCGXlOaObMmXr77bcHvw75QDcAwOiXkRCaMGECVz8AgK+UkeeEjhw5ovLyclVWVuruu+/WJ598csF9e3p6lEqlhmwAgLEh7SE0f/58vfDCC3rzzTf1/PPPq62tTYsWLVJHR8d596+rq1MsFhvcpk6dmu6WAAA5Ku0hVF1drbvuukuzZ8/W0qVL9frrr0uStm3bdt79N2zYoGQyObi1tLSkuyUAQI7K+JtVL7/8cs2ePVtHjhw57/3RaFTRaDTTbQAAclDG3yfU09OjDz/8UIlEItOHAgDkmbSH0KOPPqrGxkY1NTXpX//6l77//e8rlUqppqYm3YcCAOS5tP857vPPP9c999yj48eP66qrrtKCBQu0b98+VVRUpPtQAIA8F3E5Ns0ulUoFDTXMdXfccYd3zS9/+UvvmpBXFw4MDHjXSAp6OX3IINfS0lLvmlAhz09++OGH3jUh5yFkGGlnZ6d3jRR2Hk6cOOFd09XV5V0T8vsh9HnnceP8/1jU3NzsXXPttdd614QKGXwaGhPJZFLFxcUX3YfZcQAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMyMmgGm2RzKN3PmTO+a3bt3e9eEDLlMJpPeNSFDGiVpwgT/IeyTJk3yrgkZchmyHqSwxzRx4sSgY/k6ffp0Vo4TKmQdjR8/3rsmZChryPdVknp7e71rQj4x4OjRo941M2bM8K7JNgaYAgByGiEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADATNho2SzxmYSczWHgL730kndNNBr1rmlra/Ou+drXvuZdc+bMGe8aKeych0yCDpm0HDpFu6enx7vm1KlT3jXZekyh5yGkLlvTxEOOU1BQEHSskLqQidiJRMK7ZunSpd41kvT222971/hOSHfOjfj3A1dCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzOTsANNx48Z5DVHs7+/3PkbocMcrr7zSuyZkyGVIfxMm+H9LfYcTnjN58mTvmtbWVu+akAGrIetBCjvnIUMuQ855yMDY0DUecv5Cvk8lJSXeNSHr7sSJE941Utiw1OPHj3vXhAy0vf/++71rpLABpgMDA0HHGgmuhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJjJ2QGmIcMafS1dujSoLmSA4tGjR71rYrGYd03IANPQIZfJZNK75uTJk941IcM+Q4eyhgySDFmr2Vjfl3KckDURMsC0ra3Nu+bqq6/2rgn5vkph6yhk2GdXV5d3zW233eZdk4u4EgIAmCGEAABmvENoz549WrZsmcrLyxWJRPTqq68Oud85p9raWpWXl6uwsFBLlizR4cOH09UvAGAU8Q6hrq4uzZkzR1u2bDnv/U8++aQ2b96sLVu2aP/+/YrH47r11lvV2dl5yc0CAEYX72exq6urVV1dfd77nHN6+umntXHjRq1YsUKStG3bNpWVlWn79u164IEHLq1bAMCoktbnhJqamtTW1qaqqqrB26LRqBYvXqy9e/eet6anp0epVGrIBgAYG9IaQudebllWVjbk9rKysgu+FLOurk6xWGxwmzp1ajpbAgDksIy8Ou7L7zFwzl3wfQcbNmxQMpkc3FpaWjLREgAgB6X1zarxeFzS2SuiRCIxeHt7e/uwq6NzotGootFoOtsAAOSJtF4JVVZWKh6Pq76+fvC23t5eNTY2atGiRek8FABgFPC+Ejp58qQ+/vjjwa+bmpr03nvvqaSkRNOmTdO6deu0adMmTZ8+XdOnT9emTZs0adIk3XvvvWltHACQ/7xD6N1339XNN988+PX69eslSTU1NfrDH/6gxx57TN3d3XrooYd04sQJzZ8/X2+99ZaKiorS1zUAYFSIuGxNUhyhVCoVNLgzxK9//eugulWrVnnXhAwwLSkp8a4JGWAaMnBRChtgGjJIMqS/0KGsIXUhP0Kh/WVLtvrr7+/3rpk0aZJ3TcjPRajTp09n5TihryQOeQ6+t7c36FjJZFLFxcUX3YfZcQAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM9kbLZuDFi5cGFQXMvk3ZHJtyPToK6+80rumra3Nu0YKm4g9bpz/v3tCp3yHyNZE7GxN6w4VssZDHlPIz0V3d7d3zf//pGcfBQUF3jUhE/NDpFKpoLpvfOMb3jUffPBB0LFGgishAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZsb0ANOZM2cG1Z06dcq7JmT4ZLYGhJ45c8a7RpL6+vq8ayZM8F9y2Roqeil1o022hrKGrKGQn7+Qnwspe8NzQ9b4pEmTgo713e9+17uGAaYAgFGJEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAmVEzwDQWi3nXhA4A7Ojo8K4JGdzZ09PjXRMiZFCqJBUUFHjXhJyHELk+iDRkoGbIkMuQGil84KevkO9TYWGhd000GvWukcKH+/rq7e31runu7g461m233eZd86tf/SroWCPBlRAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzo2aA6cKFC71rTp8+HXSsgYEB75qQAYohg0X/97//ZeU4kjRx4kTvmpChrKH95bJsDVgNPU5fX593Tch6CBmCW1JS4l3zwQcfeNdI0owZM7xrQn4/hAxTDhl6Kknf/va3g+oyhSshAIAZQggAYMY7hPbs2aNly5apvLxckUhEr7766pD7V61apUgkMmRbsGBBuvoFAIwi3iHU1dWlOXPmaMuWLRfc5/bbb1dra+vgtnPnzktqEgAwOnm/MKG6ulrV1dUX3ScajSoejwc3BQAYGzLynFBDQ4NKS0s1Y8YM3XfffWpvb7/gvj09PUqlUkM2AMDYkPYQqq6u1osvvqhdu3bpqaee0v79+3XLLbdc8KW5dXV1isVig9vUqVPT3RIAIEel/X1CK1euHPzvWbNmad68eaqoqNDrr7+uFStWDNt/w4YNWr9+/eDXqVSKIAKAMSLjb1ZNJBKqqKjQkSNHznt/NBoNeiMnACD/Zfx9Qh0dHWppaVEikcj0oQAAecb7SujkyZP6+OOPB79uamrSe++9p5KSEpWUlKi2tlZ33XWXEomEPv30U/30pz/V5MmTdeedd6a1cQBA/vMOoXfffVc333zz4Nfnns+pqanRs88+q0OHDumFF17QF198oUQioZtvvlk7duxQUVFR+roGAIwK3iG0ZMkSOecueP+bb755SQ2FWrx4sclxRypkWOrXv/5175rf/e533jUhAxcl6f777/euaWpq8q4JGXIZ+piyJWSw6MV+7tItZGhsSE1/f793zRVXXOFds2bNGu8aSdq1a5d3TbbOXXd3t3eNJE2ZMiWoLlOYHQcAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMJPxT1bNlqVLl3rXhEy2lqRx4/yzu6+vL+hYvlpbW71rjh49moFOzu/MmTPeNSFTtEOmVEth07dDj5WN42Rz8nbIROzCwsIMdDLcgQMHsnIcKWwNhfxchE6KT6VS3jXz5s3z2r+/v18HDx4c0b5cCQEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADCTswNML7vsMq+Bjddcc433MUIHAIYMkgwZehpiwgT/b2lPT08GOjm/bA7UHG1Czl3o+c7Weg35GQx5TCHDVUOVlJR414QMUw59TMXFxd41Dz/8sNf+3d3duv/++0e0L1dCAAAzhBAAwAwhBAAwQwgBAMwQQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzOTsAFNfL730knfNtddeG3Ss2bNne9f897//9a6pqKjwrvn3v//tXVNWVuZdEypk+GTIMM1sDqwMeUyhw3N9ZWsQqZS9cx4yQLirqyvoWH//+9+9a0IGhBYWFnrXnDlzxrtGkv785z9712zbts1rf5+1wJUQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAwQwgBAMwQQgAAMzk7wPT06dNe+69evTpDnQw3YYL/aevr68tAJ+mxcePGrB0rZKBmyLDPkKGiUthwzNBj+QrpLZtyfdBsiKVLl1q3MOpxJQQAMEMIAQDMeIVQXV2drr/+ehUVFam0tFTLly/XRx99NGQf55xqa2tVXl6uwsJCLVmyRIcPH05r0wCA0cErhBobG7V69Wrt27dP9fX16uvrU1VV1ZAPjHryySe1efNmbdmyRfv371c8Htett96qzs7OtDcPAMhvXs+wv/HGG0O+3rp1q0pLS3XgwAHddNNNcs7p6aef1saNG7VixQpJZz+Rr6ysTNu3b9cDDzyQvs4BAHnvkp4TSiaTkqSSkhJJUlNTk9ra2lRVVTW4TzQa1eLFi7V3797z/j96enqUSqWGbACAsSE4hJxzWr9+vW644QbNmjVLktTW1iZJKisrG7JvWVnZ4H1fVldXp1gsNrhNnTo1tCUAQJ4JDqE1a9bo/fff10svvTTsvi+/n8E5d8H3OGzYsEHJZHJwa2lpCW0JAJBngt6sunbtWr322mvas2ePpkyZMnh7PB6XdPaKKJFIDN7e3t4+7OronGg0qmg0GtIGACDPeV0JOee0Zs0avfzyy9q1a5cqKyuH3F9ZWal4PK76+vrB23p7e9XY2KhFixalp2MAwKjhdSW0evVqbd++XX/9619VVFQ0+DxPLBZTYWGhIpGI1q1bp02bNmn69OmaPn26Nm3apEmTJunee+/NyAMAAOQvrxB69tlnJUlLliwZcvvWrVu1atUqSdJjjz2m7u5uPfTQQzpx4oTmz5+vt956S0VFRWlpGAAweniF0EgGNUYiEdXW1qq2tja0pyDZGowp5fYw0hCnTp3KWt348eO9a0IGxoZ+b7M1LDWkJmSAacjPhRR2HkKOFfKzlK2BsaNVyDryrXHOjfj7xOw4AIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAICZoE9WzUUhU39DpslKYdOC+/v7vWsKCgq8a86cOeNdc8UVV3jXSGc/sNBXSH8h07pDp2jnspD1GjpxOlvrNWSqelNTk3dNNoVOLvcV+r3N1tT3keJKCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgJlRM8A0RDaHO4YIGfYZ4lvf+lZQXcjg09LSUu+akpIS7xrkh1Qq5V1TXFzsXTNt2jTvGklqbm72rgkZNJut3ym5iCshAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZiIudIpnhqRSKcViMes2csK4cf7/RhgYGPCumTRpkneNJFVWVnrXhAwwveaaa7xrQnqTpKKiIu+agoKCoGP56unp8a4JGRAqSV1dXd417733nnfN0aNHvWs+//xz75ovvvjCuwaXLplMfuXAWa6EAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmGGAKQAgIxhgCgDIaYQQAMCMVwjV1dXp+uuvV1FRkUpLS7V8+XJ99NFHQ/ZZtWqVIpHIkG3BggVpbRoAMDp4hVBjY6NWr16tffv2qb6+Xn19faqqqhr2AVi33367WltbB7edO3emtWkAwOgwwWfnN954Y8jXW7duVWlpqQ4cOKCbbrpp8PZoNKp4PJ6eDgEAo9YlPSeUTCYlSSUlJUNub2hoUGlpqWbMmKH77rtP7e3tF/x/9PT0KJVKDdkAAGND8Eu0nXO64447dOLECb3zzjuDt+/YsUNXXHGFKioq1NTUpJ/97Gfq6+vTgQMHFI1Gh/1/amtr9fOf/zz8EQAActJIXqItF+ihhx5yFRUVrqWl5aL7HTt2zBUUFLi//OUv573/9OnTLplMDm4tLS1OEhsbGxtbnm/JZPIrs8TrOaFz1q5dq9dee0179uzRlClTLrpvIpFQRUWFjhw5ct77o9Hoea+QAACjn1cIOee0du1avfLKK2poaFBlZeVX1nR0dKilpUWJRCK4SQDA6OT1woTVq1frj3/8o7Zv366ioiK1tbWpra1N3d3dkqSTJ0/q0Ucf1T//+U99+umnamho0LJlyzR58mTdeeedGXkAAIA85vM8kC7wd7+tW7c655w7deqUq6qqcldddZUrKChw06ZNczU1Na65uXnEx0gmk+Z/x2RjY2Nju/RtJM8JMcAUAJARDDAFAOQ0QggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAIAZQggAYIYQAgCYIYQAAGYIIQCAGUIIAGCGEAIAmCGEAABmCCEAgBlCCABghhACAJghhAAAZgghAICZnAsh55x1CwCANBjJ7/OcC6HOzk7rFgAAaTCS3+cRl2OXHgMDAzp27JiKiooUiUSG3JdKpTR16lS1tLSouLjYqEN7nIezOA9ncR7O4jyclQvnwTmnzs5OlZeXa9y4i1/rTMhSTyM2btw4TZky5aL7FBcXj+lFdg7n4SzOw1mch7M4D2dZn4dYLDai/XLuz3EAgLGDEAIAmMmrEIpGo3r88ccVjUatWzHFeTiL83AW5+EszsNZ+XYecu6FCQCAsSOvroQAAKMLIQQAMEMIAQDMEEIAADN5FULPPPOMKisrddlll2nu3Ll65513rFvKqtraWkUikSFbPB63bivj9uzZo2XLlqm8vFyRSESvvvrqkPudc6qtrVV5ebkKCwu1ZMkSHT582KbZDPqq87Bq1aph62PBggU2zWZIXV2drr/+ehUVFam0tFTLly/XRx99NGSfsbAeRnIe8mU95E0I7dixQ+vWrdPGjRt18OBB3XjjjaqurlZzc7N1a1k1c+ZMtba2Dm6HDh2ybinjurq6NGfOHG3ZsuW89z/55JPavHmztmzZov379ysej+vWW28ddXMIv+o8SNLtt98+ZH3s3Lkzix1mXmNjo1avXq19+/apvr5efX19qqqqUldX1+A+Y2E9jOQ8SHmyHlye+M53vuMefPDBIbd985vfdD/5yU+MOsq+xx9/3M2ZM8e6DVOS3CuvvDL49cDAgIvH4+6JJ54YvO306dMuFou53/72twYdZseXz4NzztXU1Lg77rjDpB8r7e3tTpJrbGx0zo3d9fDl8+Bc/qyHvLgS6u3t1YEDB1RVVTXk9qqqKu3du9eoKxtHjhxReXm5Kisrdffdd+uTTz6xbslUU1OT2trahqyNaDSqxYsXj7m1IUkNDQ0qLS3VjBkzdN9996m9vd26pYxKJpOSpJKSEkljdz18+Tyckw/rIS9C6Pjx4+rv71dZWdmQ28vKytTW1mbUVfbNnz9fL7zwgt588009//zzamtr06JFi9TR0WHdmplz3/+xvjYkqbq6Wi+++KJ27dqlp556Svv379ctt9yinp4e69Yywjmn9evX64YbbtCsWbMkjc31cL7zIOXPesi5KdoX8+WPdnDODbttNKuurh7879mzZ2vhwoW65pprtG3bNq1fv96wM3tjfW1I0sqVKwf/e9asWZo3b54qKir0+uuva8WKFYadZcaaNWv0/vvv6x//+Mew+8bSerjQeciX9ZAXV0KTJ0/W+PHjh/1Lpr29fdi/eMaSyy+/XLNnz9aRI0esWzFz7tWBrI3hEomEKioqRuX6WLt2rV577TXt3r17yEe/jLX1cKHzcD65uh7yIoQmTpyouXPnqr6+fsjt9fX1WrRokVFX9np6evThhx8qkUhYt2KmsrJS8Xh8yNro7e1VY2PjmF4bktTR0aGWlpZRtT6cc1qzZo1efvll7dq1S5WVlUPuHyvr4avOw/nk7HowfFGElz/96U+uoKDA/f73v3cffPCBW7dunbv88svdp59+at1a1jzyyCOuoaHBffLJJ27fvn3ue9/7nisqKhr156Czs9MdPHjQHTx40ElymzdvdgcPHnSfffaZc865J554wsViMffyyy+7Q4cOuXvuucclEgmXSqWMO0+vi52Hzs5O98gjj7i9e/e6pqYmt3v3brdw4UJ39dVXj6rz8KMf/cjFYjHX0NDgWltbB7dTp04N7jMW1sNXnYd8Wg95E0LOOfeb3/zGVVRUuIkTJ7rrrrtuyMsRx4KVK1e6RCLhCgoKXHl5uVuxYoU7fPiwdVsZt3v3bidp2FZTU+OcO/uy3Mcff9zF43EXjUbdTTfd5A4dOmTbdAZc7DycOnXKVVVVuauuusoVFBS4adOmuZqaGtfc3Gzddlqd7/FLclu3bh3cZyysh686D/m0HvgoBwCAmbx4TggAMDoRQgAAM4QQAMAMIQQAMEMIAQDMEEIAADOEEADADCEEADBDCAEAzBBCAAAzhBAAwAwhBAAw839XjIi3rLD7uAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data often requires preprocessing before it's suitable for training machine learning algorithms. Transforms help manipulate the data into the appropriate format.\n",
    "\n",
    "TorchVision datasets include two parameters: 'transform' for modifying features and 'target_transform' for altering labels. These accept callables containing transformation logic. The torchvision.transforms module provides many ready-to-use transforms.\n",
    "\n",
    "FashionMNIST features are in PIL Image format with integer labels. For training, we need normalized tensor features and one-hot encoded tensor labels. We use ToTensor and Lambda transforms to achieve these conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Tensor()\n",
    "\n",
    "- ToTensor converts a PIL image or NumPy ndarray into a FloatTensor. and scales the images pixel intensity values in the range [0., 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, Lambda transforms apply custom, user-defined transformations to data within the torchvision.transforms module. This feature offers flexibility for preprocessing steps not covered by torchvision's standard transformations. The Lambda transform takes a function as an argument and applies it to each dataset element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A  neural network consists of interconnected nodes (neurons) that process and learn from data through weighted connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `torch.nn` is a module in PyTorch that provides essential tools for building and training neural networks. It includes pre-defined layers, loss functions, and utilities to streamline the creation, customization, and optimization of deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Device for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([5])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Flatten, nn.Linear, nn.ReLU, nn.Sequential, nn.Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Flatten converts multi-dimensional input tensors into 1-dimensional tensors in neural networks, particularly in PyTorch.\n",
    "\n",
    "nn.Linear, also called a fully connected or dense layer, is a key component in neural networks. It performs linear transformations on input data, mapping it to an output space.\n",
    "\n",
    "nn.ReLU (Rectified Linear Unit) is a popular activation function that introduces non-linearity into neural networks, essential for learning complex patterns.\n",
    "\n",
    "nn.Sequential is a PyTorch container module that allows stacking layers in order, useful for building simple feed-forward neural networks where each layer's output becomes the next layer's input.\n",
    "\n",
    "nn.Softmax is an activation function used in the final layer of classification models. It transforms raw output scores (logits) into probabilities, making them easier to interpret and use for decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n",
      "torch.Size([3, 20])\n",
      "Before ReLU:  tensor([[ 0.1667,  0.3507, -0.2780, -0.8173,  0.0154,  0.2413, -0.2055,  0.2247,\n",
      "          0.0712,  0.3840, -0.4055,  0.4587, -0.5476,  0.3107,  0.5664,  0.2306,\n",
      "         -0.2405,  0.4208,  0.4110, -0.5400],\n",
      "        [ 0.1677,  0.3702, -0.1273, -0.3663, -0.1930, -0.0055, -0.3172,  0.2557,\n",
      "         -0.3115,  0.3010, -0.7132,  0.3746, -0.7390,  0.2872,  0.1875, -0.2551,\n",
      "         -0.2621,  0.5194,  0.7427, -0.7259],\n",
      "        [ 0.0276,  0.4775,  0.0320, -0.6870,  0.1666,  0.0600, -0.2279,  0.1717,\n",
      "         -0.2402,  0.4805, -0.6330,  0.2738, -0.2006, -0.0323,  0.4694, -0.3224,\n",
      "         -0.2778,  0.4323,  0.4533, -0.4107]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.1667, 0.3507, 0.0000, 0.0000, 0.0154, 0.2413, 0.0000, 0.2247, 0.0712,\n",
      "         0.3840, 0.0000, 0.4587, 0.0000, 0.3107, 0.5664, 0.2306, 0.0000, 0.4208,\n",
      "         0.4110, 0.0000],\n",
      "        [0.1677, 0.3702, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2557, 0.0000,\n",
      "         0.3010, 0.0000, 0.3746, 0.0000, 0.2872, 0.1875, 0.0000, 0.0000, 0.5194,\n",
      "         0.7427, 0.0000],\n",
      "        [0.0276, 0.4775, 0.0320, 0.0000, 0.1666, 0.0600, 0.0000, 0.1717, 0.0000,\n",
      "         0.4805, 0.0000, 0.2738, 0.0000, 0.0000, 0.4694, 0.0000, 0.0000, 0.4323,\n",
      "         0.4533, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "flatten =  nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "#nn.Linear \n",
    "layer1 = nn.Linear (in_features=28*28, out_features = 20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n",
    "#nn.ReLU \n",
    "print(f\"Before ReLU:  {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n",
    "#nn.Sequential \n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "#nn.Softmax\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0028,  0.0232,  0.0055,  ..., -0.0239,  0.0258, -0.0075],\n",
      "        [-0.0210, -0.0166,  0.0223,  ..., -0.0144,  0.0143, -0.0240]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0185, -0.0147], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0351,  0.0166,  0.0196,  ..., -0.0130,  0.0286,  0.0379],\n",
      "        [ 0.0120, -0.0275,  0.0270,  ..., -0.0050,  0.0123, -0.0168]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0294, -0.0436], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0144, -0.0286,  0.0203,  ...,  0.0107, -0.0381,  0.0192],\n",
      "        [ 0.0303, -0.0224,  0.0211,  ...,  0.0183,  0.0282,  0.0191]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0049, -0.0267], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "  print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation with torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad= True)\n",
    "b = torch.randn(3, requires_grad = True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x32d556770>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x32ce9f280>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0220, 0.1153, 0.0273],\n",
      "        [0.0220, 0.1153, 0.0273],\n",
      "        [0.0220, 0.1153, 0.0273],\n",
      "        [0.0220, 0.1153, 0.0273],\n",
      "        [0.0220, 0.1153, 0.0273]])\n",
      "tensor([0.0220, 0.1153, 0.0273])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "\n",
    " We can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n",
    "\n",
    "We can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling Gradient Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, all tensors with requires_grad=True track their computational history and support gradient computation. However, in some cases, such as when applying a trained model to input data, we only need forward computations through the network. To stop tracking computations, we can wrap our code in a torch.no_grad() block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "  z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Another way to achieve the same result is to use the detach() method on the tensor:\n",
    "z = torch.matmul(x,w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Reading: Tensor Gradients and Jacobian Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp+1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisite Code\n",
    "# we load the code from the previous sections on Datasets and Data Loaders and Build Model.\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Implementation\n",
    "We define train_loop that loops over our optimization code, and test_loop that evaluates the models performance against our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the loss function and optimizer, and pass it to train_loop and test_loop. Feel free to increase the number of epochs to track the models improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.307523  [   64/60000]\n",
      "loss: 2.293025  [ 6464/60000]\n",
      "loss: 2.278761  [12864/60000]\n",
      "loss: 2.272024  [19264/60000]\n",
      "loss: 2.249800  [25664/60000]\n",
      "loss: 2.232403  [32064/60000]\n",
      "loss: 2.234823  [38464/60000]\n",
      "loss: 2.202367  [44864/60000]\n",
      "loss: 2.204806  [51264/60000]\n",
      "loss: 2.186119  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 2.170495 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.175395  [   64/60000]\n",
      "loss: 2.168750  [ 6464/60000]\n",
      "loss: 2.118644  [12864/60000]\n",
      "loss: 2.136604  [19264/60000]\n",
      "loss: 2.081940  [25664/60000]\n",
      "loss: 2.033845  [32064/60000]\n",
      "loss: 2.061865  [38464/60000]\n",
      "loss: 1.988104  [44864/60000]\n",
      "loss: 1.995690  [51264/60000]\n",
      "loss: 1.935891  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 1.924453 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.945374  [   64/60000]\n",
      "loss: 1.926251  [ 6464/60000]\n",
      "loss: 1.810330  [12864/60000]\n",
      "loss: 1.854075  [19264/60000]\n",
      "loss: 1.738836  [25664/60000]\n",
      "loss: 1.690559  [32064/60000]\n",
      "loss: 1.717331  [38464/60000]\n",
      "loss: 1.612887  [44864/60000]\n",
      "loss: 1.633867  [51264/60000]\n",
      "loss: 1.536189  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.547447 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.602855  [   64/60000]\n",
      "loss: 1.576373  [ 6464/60000]\n",
      "loss: 1.418135  [12864/60000]\n",
      "loss: 1.493214  [19264/60000]\n",
      "loss: 1.366269  [25664/60000]\n",
      "loss: 1.363454  [32064/60000]\n",
      "loss: 1.382143  [38464/60000]\n",
      "loss: 1.300463  [44864/60000]\n",
      "loss: 1.332280  [51264/60000]\n",
      "loss: 1.240643  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: 1.264880 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.335331  [   64/60000]\n",
      "loss: 1.323312  [ 6464/60000]\n",
      "loss: 1.148906  [12864/60000]\n",
      "loss: 1.258409  [19264/60000]\n",
      "loss: 1.128524  [25664/60000]\n",
      "loss: 1.156899  [32064/60000]\n",
      "loss: 1.182206  [38464/60000]\n",
      "loss: 1.113657  [44864/60000]\n",
      "loss: 1.148987  [51264/60000]\n",
      "loss: 1.076948  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.095468 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.161201  [   64/60000]\n",
      "loss: 1.169354  [ 6464/60000]\n",
      "loss: 0.977062  [12864/60000]\n",
      "loss: 1.116431  [19264/60000]\n",
      "loss: 0.987342  [25664/60000]\n",
      "loss: 1.022053  [32064/60000]\n",
      "loss: 1.062024  [38464/60000]\n",
      "loss: 0.997779  [44864/60000]\n",
      "loss: 1.031729  [51264/60000]\n",
      "loss: 0.977398  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.988463 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.042084  [   64/60000]\n",
      "loss: 1.071430  [ 6464/60000]\n",
      "loss: 0.861529  [12864/60000]\n",
      "loss: 1.022769  [19264/60000]\n",
      "loss: 0.899825  [25664/60000]\n",
      "loss: 0.927948  [32064/60000]\n",
      "loss: 0.984016  [38464/60000]\n",
      "loss: 0.922962  [44864/60000]\n",
      "loss: 0.951751  [51264/60000]\n",
      "loss: 0.910872  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.915845 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.954888  [   64/60000]\n",
      "loss: 1.003198  [ 6464/60000]\n",
      "loss: 0.778875  [12864/60000]\n",
      "loss: 0.956080  [19264/60000]\n",
      "loss: 0.841538  [25664/60000]\n",
      "loss: 0.858793  [32064/60000]\n",
      "loss: 0.928502  [38464/60000]\n",
      "loss: 0.872330  [44864/60000]\n",
      "loss: 0.894210  [51264/60000]\n",
      "loss: 0.862348  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.863287 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.887583  [   64/60000]\n",
      "loss: 0.951493  [ 6464/60000]\n",
      "loss: 0.717086  [12864/60000]\n",
      "loss: 0.905932  [19264/60000]\n",
      "loss: 0.799800  [25664/60000]\n",
      "loss: 0.806408  [32064/60000]\n",
      "loss: 0.885762  [38464/60000]\n",
      "loss: 0.836495  [44864/60000]\n",
      "loss: 0.851158  [51264/60000]\n",
      "loss: 0.824279  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.823073 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.833544  [   64/60000]\n",
      "loss: 0.909459  [ 6464/60000]\n",
      "loss: 0.668952  [12864/60000]\n",
      "loss: 0.866589  [19264/60000]\n",
      "loss: 0.767943  [25664/60000]\n",
      "loss: 0.765953  [32064/60000]\n",
      "loss: 0.850803  [38464/60000]\n",
      "loss: 0.809950  [44864/60000]\n",
      "loss: 0.817971  [51264/60000]\n",
      "loss: 0.793162  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.8%, Avg loss: 0.790938 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load the Model\n",
    "In this section we will look at how to persist model state with saving, loading and running model predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Model Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models store the learned parameters in an internal state dictionary, called state_dict. These can be persisted via the torch.save method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights='IMAGENET1K_V1')\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load model weights, you need to create an instance of the same model first, and then load the parameters using load_state_dict() method.\n",
    "\n",
    "In the code below, we set weights_only=True to limit the functions executed during unpickling to only those necessary for loading weights. Using weights_only=True is considered a best practice when loading weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model\n",
    "model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models with Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading model weights, we needed to instantiate the model class first, because the class defines the structure of a network. We might want to save the structure of this class together with the model, in which case we can pass model (and not model.state_dict()) to the saving function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load the model as demonstrated below.\n",
    "\n",
    "As described in Saving and loading torch.nn.Modules, saving state_dict``s is considered the best practice. However, below we use ``weights_only=False because this involves loading the model, which is a legacy use case for torch.save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "This approach uses Python pickle module when serializing the model, thus it relies on the actual class definition to be available when loading the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Models\n",
    "\n",
    "Here we will achieve the same result using pytorch models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking over this code, we can spot some structural similarities with the diagram above.\n",
    "\n",
    "This demonstrates the structure of a typical PyTorch model:\n",
    "- It inherits from torch.nn.Module - modules may be nested - in fact, even the Conv2d and Linear layer classes inherit from torch.nn.Module.\n",
    "- A model will have an __init__() function, where it instantiates its layers, and loads any data artifacts it might need (e.g., an NLP model might load a vocabulary).\n",
    "- A model will have a forward() function. This is where the actual computation happens: An input is passed through the network layers and various functions to generate an output.\n",
    "- Other than that, you can build out your model class like any other Python class, adding whatever properties and methods you need to support your models computation.\n",
    "\n",
    "Lets instantiate this object and run a sample input through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Image batch shape:\n",
      "torch.Size([1, 1, 32, 32])\n",
      "\n",
      "Raw output:\n",
      "tensor([[ 0.0851, -0.0153,  0.0974, -0.0303,  0.0129,  0.1163, -0.0890,  0.1027,\n",
      "          0.1006, -0.0239]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)                         # what does the object tell us about itself?\n",
    "\n",
    "input = torch.rand(1, 1, 32, 32)   # stand-in for a 32x32 black & white image\n",
    "print('\\nImage batch shape:')\n",
    "print(input.shape)\n",
    "\n",
    "output = net(input)                # we don't call forward() directly\n",
    "print('\\nRaw output:')\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few important things happening above:\n",
    "\n",
    "- First, we instantiate the LeNet class, and we print the net object. A subclass of torch.nn.Module will report the layers it has created and their shapes and parameters. This can provide a handy overview of a model if you want to get the gist of its processing.\n",
    "\n",
    "- Below that, we create a dummy input representing a 32x32 image with 1 color channel. Normally, you would load an image tile and convert it to a tensor of this shape.\n",
    "\n",
    "- You may have noticed an extra dimension to our tensor - the batch dimension. PyTorch models assume they are working on batches of data - for example, a batch of 16 of our image tiles would have the shape (16, 1, 32, 32). Since were only using one image, we create a batch of 1 with shape (1, 1, 32, 32).\n",
    "\n",
    "- We ask the model for an inference by calling it like a function: net(input). The output of this call represents the models confidence that the input represents a particular digit. (Since this instance of the model hasnt learned anything yet, we shouldnt expect to see any signal in the output.) Looking at the shape of output, we can see that it also has a batch dimension, the size of which should always match the input batch dimension. If we had passed in an input batch of 16 instances, output would have a shape of (16, 10).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, were going to demonstrate using one of the ready-to-download, open-access datasets from TorchVision, how to transform the images for consumption by your model, and how to use the DataLoader to feed batches of data to your model.\n",
    "\n",
    "The first thing we need to do is transform our incoming images into a PyTorch tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we specify two transformations for our input:\n",
    "\n",
    "transforms.ToTensor() converts images loaded by Pillow into PyTorch tensors.\n",
    "\n",
    "transforms.Normalize() adjusts the values of the tensor so that their average is zero and their standard deviation is 1.0. Most activation functions have their strongest gradients around x = 0, so centering our data there can speed learning. The values passed to the transform are the means (first tuple) and the standard deviations (second tuple) of the rgb values of the images in the dataset. You can calculate these values yourself by running these few lines of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.data import ConcatDataset transform = transforms.Compose([transforms.ToTensor()]) trainset = torchvision.datasets.CIFAR10(root=./data, train=True,\n",
    "\n",
    "download=True, transform=transform)\n",
    "\n",
    "#stack all train images together into a tensor of shape #(50000, 3, 32, 32) x = torch.stack([sample[0] for sample in ConcatDataset([trainset])])\n",
    "\n",
    "#get the mean of each channel mean = torch.mean(x, dim=(0,2,3)) #tensor([0.4914, 0.4822, 0.4465]) std = torch.std(x, dim=(0,2,3)) #tensor([0.2470, 0.2435, 0.2616])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more transforms available, including cropping, centering, rotation, and reflection.\n",
    "\n",
    "Next, well create an instance of the CIFAR10 dataset. This is a set of 32x32 color image tiles representing 10 classes of objects: 6 of animals (bird, cat, deer, dog, frog, horse) and 4 of vehicles (airplane, automobile, ship, truck):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root= './data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of creating a dataset object in PyTorch. Downloadable datasets (like CIFAR-10 above) are subclasses of torch.utils.data.Dataset. Dataset classes in PyTorch include the downloadable datasets in TorchVision, Torchtext, and TorchAudio, as well as utility dataset classes such as torchvision.datasets.ImageFolder, which will read a folder of labeled images. You can also create your own subclasses of Dataset.\n",
    "\n",
    "When we instantiate our dataset, we need to tell it a few things:\n",
    "\n",
    "- The filesystem path to where we want the data to go.\n",
    "\n",
    "- Whether or not we are using this set for training; most datasets will be split into training and test subsets.\n",
    "\n",
    "- Whether we would like to download the dataset if we havent already.\n",
    "\n",
    "- The transformations we want to apply to the data.\n",
    "\n",
    "- Once your dataset is ready, you can give it to the DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A  Dataset subclass wraps access to the data, and is specialized to the type of data its serving. The DataLoader knows nothing about the data, but organizes the input tensors served by the Dataset into batches with the parameters you specify.\n",
    "\n",
    "In the example above, weve asked a DataLoader to give us batches of 4 images from trainset, randomizing their order (shuffle=True), and we told it to spin up two workers to load data from disk.\n",
    "\n",
    "Its good practice to visualize the batches your DataLoader serves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane  deer   dog  frog\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all the pieces together, and train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll need training and test datasets. If you haven't already, run the cell below to make sure the dataset is downloaded. (It may take a minute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root= './data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run our check on the output from DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " frog truck   dog   car\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "  img = img/ 2 + 0.5  # unnormalize\n",
    "  npimg = img.numpy()\n",
    "  plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "#get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "#show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "#print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model we'll train.If it looks familiar, that's because it's a variant of LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last ingredients we need are a loss function and an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function, as discussed earlier in this video, is a measure of how far from our ideal output the models prediction was. Cross-entropy loss is a typical loss function for classification models like ours.\n",
    "\n",
    "The optimizer is what drives the learning. Here we have created an optimizer that implements stochastic gradient descent, one of the more straightforward optimization algorithms. Besides parameters of the algorithm, like the learning rate (lr) and momentum, we also pass in net.parameters(), which is a collection of all the learning weights in the model - which is what the optimizer adjusts.\n",
    "\n",
    "Finally, all of this is assembled into the training loop. Go ahead and run this cell, as it will likely take a few minutes to execute:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.227\n",
      "[1,  4000] loss: 1.887\n",
      "[1,  6000] loss: 1.699\n",
      "[1,  8000] loss: 1.600\n",
      "[1, 10000] loss: 1.520\n",
      "[1, 12000] loss: 1.466\n",
      "[2,  2000] loss: 1.394\n",
      "[2,  4000] loss: 1.363\n",
      "[2,  6000] loss: 1.323\n",
      "[2,  8000] loss: 1.290\n",
      "[2, 10000] loss: 1.293\n",
      "[2, 12000] loss: 1.260\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2): # loop over the dataset multiple times\n",
    "\n",
    "\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "    #get the inputs\n",
    "    inputs, labels = data\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    #forward+ backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 2000 == 1999: #print every 2000 mini-batches\n",
    "      print('[%d, %5d] loss: %.3f' %(epoch + 1, i+1, running_loss / 2000))\n",
    "      running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we are doing only 2 training epochs (line 1) - that is, two passes over the training dataset. Each pass has an inner loop that iterates over the training data (line 4), serving batches of transformed input images and their correct labels.\n",
    "\n",
    "- Zeroing the gradients (line 9) is an important step. Gradients are accumulated over a batch; if we do not reset them for every batch, they will keep accumulating, which will provide incorrect gradient values, making learning impossible.\n",
    "\n",
    "- In line 12, we ask the model for its predictions on this batch. In the following line (13), we compute the loss - the difference between outputs (the model prediction) and labels (the correct output).\n",
    "\n",
    "- In line 14, we do the backward() pass, and calculate the gradients that will direct the learning.\n",
    "\n",
    "- In line 15, the optimizer performs one learning step - it uses the gradients from the backward() call to nudge the learning weights in the direction it thinks will reduce the loss.\n",
    "\n",
    "- The remainder of the loop does some light reporting on the epoch number, how many training instances have been completed, and what the collected loss is over the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the loss is monotonically descending, indicating that our model is continuing to improve its performance on the training dataset.\n",
    "\n",
    "As a final step, we should check that the model is actually doing general learning, and not simply memorizing the dataset. This is called overfitting, and usually indicates that the dataset is too small (not enough examples for general learning), or that the model has more learning parameters than it needs to correctly model the dataset.\n",
    "\n",
    "This is the reason datasets are split into training and test subsets - to test the generality of the model, we ask it to make predictions on data it hasnt trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 55 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the model is roughly 50% accurate at this point. Thats not exactly state-of-the-art, but its far better than the 10% accuracy wed expect from a random output. This demonstrates that some general learning did happen in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Models with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.Module and torch.nn.Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important behavior of torch.nn.Module is registering parameters. If a particular Module subclass has learning weights, these weights are expressed as instances of torch.nn.Parameter. The Parameter class is a subclass of torch.Tensor, with the special behavior that when they are assigned as attributes of a Module, they are added to the list of that modules parameters. These parameters may be accessed through the parameters() method on the Module class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, heres a very simple model with two linear layers and an activation function. Well create an instance of it and ask it to report on its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0269, -0.0572,  0.0371,  ..., -0.0579,  0.0048, -0.0493],\n",
      "        [ 0.0512,  0.0596,  0.0321,  ...,  0.0242, -0.0196, -0.0885],\n",
      "        [ 0.0633, -0.0711, -0.0552,  ...,  0.0622, -0.0284,  0.0644],\n",
      "        ...,\n",
      "        [ 0.0781,  0.0191, -0.0212,  ...,  0.0927, -0.0799,  0.0668],\n",
      "        [-0.0997, -0.0837, -0.0199,  ..., -0.0828, -0.0227, -0.0799],\n",
      "        [-0.0459,  0.0956, -0.0462,  ..., -0.0242, -0.0137, -0.0730]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0004, -0.0948,  0.0976, -0.0266, -0.0109, -0.0458,  0.0133, -0.0498,\n",
      "         0.0078, -0.0184,  0.0256, -0.0189, -0.0244, -0.0322, -0.0227, -0.0082,\n",
      "         0.0374,  0.0592, -0.0335, -0.0576, -0.0127,  0.0529,  0.0972,  0.0163,\n",
      "         0.0393,  0.0612, -0.0722,  0.0747,  0.0914, -0.0530, -0.0989, -0.0997,\n",
      "         0.0363,  0.0426,  0.0416,  0.0179, -0.0708, -0.0531,  0.0300, -0.0692,\n",
      "         0.0234,  0.0206,  0.0367, -0.0621,  0.0625,  0.0708,  0.0653, -0.0397,\n",
      "        -0.0793,  0.0432, -0.0329,  0.0776,  0.0824, -0.0410, -0.0527, -0.0523,\n",
      "        -0.0538,  0.0673,  0.0903,  0.0031, -0.0350,  0.0270, -0.0895,  0.0510,\n",
      "         0.0929,  0.0011,  0.0985,  0.0326,  0.0730, -0.0993,  0.0729, -0.0930,\n",
      "         0.0475,  0.0555, -0.0413, -0.0548, -0.0191,  0.0333,  0.0627,  0.0902,\n",
      "         0.0053, -0.0645,  0.0305,  0.0669, -0.0748,  0.0199, -0.0652,  0.0233,\n",
      "         0.0465, -0.0059, -0.0487,  0.0281, -0.0402, -0.0360,  0.0301, -0.0716,\n",
      "        -0.0010, -0.0073, -0.0898,  0.0604, -0.0737, -0.0805,  0.0458, -0.0976,\n",
      "         0.0571,  0.0183,  0.0472, -0.0350,  0.0334,  0.0308,  0.0240,  0.0581,\n",
      "        -0.0050, -0.0116, -0.0093,  0.0203, -0.0815, -0.0631, -0.0896, -0.0882,\n",
      "         0.0279,  0.0026,  0.0126, -0.0223, -0.0086, -0.0430,  0.0466,  0.0222,\n",
      "         0.0542,  0.0645, -0.0006, -0.0916, -0.0852,  0.0038,  0.0262, -0.0339,\n",
      "        -0.0639,  0.0656,  0.0836,  0.0573,  0.0187,  0.0745,  0.0966,  0.0603,\n",
      "        -0.0746, -0.0064, -0.0116,  0.0420, -0.0144, -0.0194,  0.0667,  0.0432,\n",
      "        -0.0256, -0.0240,  0.0486,  0.0294,  0.0526, -0.0890,  0.0945, -0.0654,\n",
      "        -0.0980,  0.0481,  0.0682, -0.0970, -0.0119, -0.0946,  0.0641,  0.0644,\n",
      "         0.0173, -0.0089,  0.0547,  0.0184, -0.0070,  0.0871, -0.0808,  0.0703,\n",
      "         0.0516, -0.0843, -0.0165, -0.0988, -0.0446,  0.0970, -0.0041,  0.0283,\n",
      "        -0.0307, -0.0155,  0.0617, -0.0667,  0.0161,  0.0790,  0.0561,  0.0376,\n",
      "        -0.0861,  0.0203,  0.0435,  0.0942,  0.0600, -0.0524,  0.0371, -0.0493],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0186,  0.0256,  0.0118,  ..., -0.0202, -0.0020,  0.0262],\n",
      "        [-0.0680,  0.0146,  0.0158,  ...,  0.0673,  0.0675,  0.0271],\n",
      "        [ 0.0362,  0.0602,  0.0296,  ...,  0.0140, -0.0139,  0.0343],\n",
      "        ...,\n",
      "        [-0.0170,  0.0220,  0.0508,  ...,  0.0182, -0.0060, -0.0042],\n",
      "        [-0.0218, -0.0280,  0.0579,  ..., -0.0669,  0.0103, -0.0374],\n",
      "        [ 0.0642,  0.0509,  0.0302,  ...,  0.0090, -0.0527,  0.0187]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0644,  0.0209,  0.0613,  0.0080,  0.0295, -0.0696, -0.0442,  0.0195,\n",
      "        -0.0544,  0.0702], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0186,  0.0256,  0.0118,  ..., -0.0202, -0.0020,  0.0262],\n",
      "        [-0.0680,  0.0146,  0.0158,  ...,  0.0673,  0.0675,  0.0271],\n",
      "        [ 0.0362,  0.0602,  0.0296,  ...,  0.0140, -0.0139,  0.0343],\n",
      "        ...,\n",
      "        [-0.0170,  0.0220,  0.0508,  ...,  0.0182, -0.0060, -0.0042],\n",
      "        [-0.0218, -0.0280,  0.0579,  ..., -0.0669,  0.0103, -0.0374],\n",
      "        [ 0.0642,  0.0509,  0.0302,  ...,  0.0090, -0.0527,  0.0187]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0644,  0.0209,  0.0613,  0.0080,  0.0295, -0.0696, -0.0442,  0.0195,\n",
      "        -0.0544,  0.0702], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the fundamental structure of a PyTorch model: there is an __init__() method that defines the layers and other components of a model, and a forward() method where the computation gets done. Note that we can print the model, or any of its submodules, to learn about its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Layer Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic type of neural network layer is a linear or fully connected layer. This is a layer where every input influences every output of the layer to a degree specified by the layers weights. If a model has m inputs and n outputs, the weights will be an m x n matrix. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.2240, 0.8502, 0.0382]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[0.4565, 0.4714, 0.2435],\n",
      "        [0.4783, 0.2244, 0.3859]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1042, -0.2300], requires_grad=True)\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[0.6165, 0.0826]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input:')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters:')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\nOutput:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do the matrix multiplication of x by the linear layers weights, and add the biases, youll find that you get the output vector y.\n",
    "\n",
    "One other important feature to note: When we checked the weights of our layer with lin.weight, it reported itself as a Parameter (which is a subclass of Tensor), and let us know that its tracking gradients with autograd. This is a default behavior for Parameter that differs from Tensor.\n",
    "\n",
    "Linear layers are used widely in deep learning models. One of the most common places youll see them is in classifier models, which will usually have one or more linear layers at the end, where the last layer will have n outputs, where n is the number of classes the classifier addresses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers are built to handle data with a high degree of spatial correlation. They are very commonly used in computer vision, where they detect close groupings of features which the compose into higher-level features. They pop up in other contexts too - for example, in NLP applications, where a words immediate context (that is, the other words nearby in the sequence) can affect the meaning of a sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets break down whats happening in the convolutional layers of this model. Starting with conv1:\n",
    "\n",
    "LeNet5 is meant to take in a 1x32x32 black & white image. The first argument to a convolutional layers constructor is the number of input channels. Here, it is 1. If we were building this model to look at 3-color channels, it would be 3.\n",
    "\n",
    "A convolutional layer is like a window that scans over the image, looking for a pattern it recognizes. These patterns are called features, and one of the parameters of a convolutional layer is the number of features we would like it to learn. This is the second argument to the constructor is the number of output features. Here, were asking our layer to learn 6 features.\n",
    "\n",
    "Just above, I likened the convolutional layer to a window - but how big is the window? The third argument is the window or kernel size. Here, the 5 means weve chosen a 5x5 kernel. (If you want a kernel with height different from width, you can specify a tuple for this argument - e.g., (3, 5) to get a 3x5 convolution kernel.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a convolutional layer is an activation map - a spatial representation of the presence of features in the input tensor. conv1 will give us an output tensor of 6x28x28; 6 is the number of features, and 28 is the height and width of our map. (The 28 comes from the fact that when scanning a 5-pixel window over a 32-pixel row, there are only 28 valid positions.)\n",
    "\n",
    "We then pass the output of the convolution through a ReLU activation function (more on activation functions later), then through a max pooling layer. The max pooling layer takes features near each other in the activation map and groups them together. It does this by reducing the tensor, merging every 2x2 group of cells in the output into a single cell, and assigning that cell the maximum value of the 4 cells that went into it. This gives us a lower-resolution version of the activation map, with dimensions 6x14x14.\n",
    "\n",
    "Our next convolutional layer, conv2, expects 6 input channels (corresponding to the 6 features sought by the first layer), has 16 output channels, and a 3x3 kernel. It puts out a 16x12x12 activation map, which is again reduced by a max pooling layer to 16x6x6. Prior to passing this output to the linear layers, it is reshaped to a 16 * 6 * 6 = 576-element vector for consumption by the next layer.\n",
    "\n",
    "There are convolutional layers for addressing 1D, 2D, and 3D tensors. There are also many more optional arguments for a conv layer constructor, including stride length(e.g., only scanning every second or every third position) in the input, padding (so you can scan out to the edges of the input), and more. See the documentation for more information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks (or RNNs) are used for sequential data - anything from time-series measurements from a scientific instrument to natural language sentences to DNA nucleotides. An RNN does this by maintaining a hidden state that acts as a sort of memory for what it has seen in the sequence so far.\n",
    "\n",
    "The internal structure of an RNN layer - or its variants, the LSTM (long short-term memory) and GRU (gated recurrent unit) - is moderately complex and beyond the scope of this video, but well show you what one looks like in action with an LSTM-based part-of-speech tagger (a type of classifier that tells you if a word is a noun, verb, etc.):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor has four arguments:\n",
    "\n",
    "vocab_size is the number of words in the input vocabulary. Each word is a one-hot vector (or unit vector) in a vocab_size-dimensional space.\n",
    "\n",
    "tagset_size is the number of tags in the output set.\n",
    "\n",
    "embedding_dim is the size of the embedding space for the vocabulary. An embedding maps a vocabulary onto a low-dimensional space, where words with similar meanings are close together in the space.\n",
    "\n",
    "hidden_dim is the size of the LSTMs memory.\n",
    "\n",
    "The input will be a sentence with the words represented as indices of one-hot vectors. The embedding layer will then map these down to an embedding_dim-dimensional space. The LSTM takes this sequence of embeddings and iterates over it, fielding an output vector of length hidden_dim. The final linear layer acts as a classifier; applying log_softmax() to the output of the final layer converts the output into a normalized set of estimated probabilities that a given word maps to a given tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers are multi-purpose networks that have taken over the state of the art in NLP with models like BERT. A discussion of transformer architecture is beyond the scope of this video, but PyTorch has a Transformer class that allows you to define the overall parameters of a transformer model - the number of attention heads, the number of encoder & decoder layers, dropout and activation functions, etc. (You can even build the BERT model from this single class, with the right parameters!) The torch.nn.Transformer class also has classes to encapsulate the individual components (TransformerEncoder, TransformerDecoder) and subcomponents (TransformerEncoderLayer, TransformerDecoderLayer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Layers and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Manipulation Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other layer types that perform important functions in models, but dont participate in the learning process themselves.\n",
    "\n",
    "Max pooling (and its twin, min pooling) reduce a tensor by combining cells, and assigning the maximum value of the input cells to the output cell (we saw this). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3224, 0.3377, 0.6741, 0.1029, 0.4638, 0.4790],\n",
      "         [0.2881, 0.5509, 0.2951, 0.4853, 0.7459, 0.7176],\n",
      "         [0.1691, 0.4626, 0.7202, 0.0369, 0.5813, 0.1516],\n",
      "         [0.2189, 0.5179, 0.0519, 0.0983, 0.5620, 0.2256],\n",
      "         [0.9773, 0.4204, 0.2714, 0.3087, 0.7568, 0.6110],\n",
      "         [0.4765, 0.8580, 0.8190, 0.9481, 0.5266, 0.3538]]])\n",
      "tensor([[[0.7202, 0.7459],\n",
      "         [0.9773, 0.9481]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely at the values above, youll see that each of the values in the maxpooled output is the maximum value of each quadrant of the 6x6 input.\n",
    "\n",
    "Normalization layers re-center and normalize the output of one layer before feeding it to another. Centering and scaling the intermediate tensors has a number of beneficial effects, such as letting you use higher learning rates without exploding/vanishing gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[19.5715, 12.8166, 19.7759, 21.2435],\n",
      "         [ 8.5956, 19.6041, 12.2709,  7.7482],\n",
      "         [ 7.8302, 16.3638, 17.3497, 16.2224],\n",
      "         [15.9994, 18.0637, 24.4571, 17.9766]]])\n",
      "tensor(15.9931)\n",
      "tensor([[[ 0.3741, -1.6978,  0.4368,  0.8869],\n",
      "         [-0.7394,  1.6137,  0.0462, -0.9205],\n",
      "         [-1.7209,  0.5004,  0.7570,  0.4636],\n",
      "         [-0.9803, -0.3327,  1.6730, -0.3600]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor(8.1956e-08, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell above, weve added a large scaling factor and offset to an input tensor; you should see the input tensors mean() somewhere in the neighborhood of 15. After running it through the normalization layer, you can see that the values are smaller, and grouped around zero - in fact, the mean should be very small (> 1e-8).\n",
    "\n",
    "This is beneficial because many activation functions (discussed below) have their strongest gradients near 0, but sometimes suffer from vanishing or exploding gradients for inputs that drive them far away from zero. Keeping the data centered around the area of steepest gradient will tend to mean faster, better learning and higher feasible learning rates.\n",
    "\n",
    "Dropout layers are a tool for encouraging sparse representations in your model - that is, pushing it to do inference with less data.\n",
    "\n",
    "Dropout layers work by randomly setting parts of the input tensor during training - dropout layers are always turned off for inference. This forces the model to learn against this masked or reduced dataset. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.1841, 0.0000],\n",
      "         [1.2755, 0.0000, 1.0831, 0.0000],\n",
      "         [0.8027, 0.5163, 0.3240, 0.0000],\n",
      "         [0.0000, 0.5399, 0.0000, 0.8007]]])\n",
      "tensor([[[0.0000, 0.0000, 0.1841, 0.5716],\n",
      "         [1.2755, 0.0000, 1.0831, 0.0000],\n",
      "         [0.8027, 0.0000, 0.3240, 0.4346],\n",
      "         [0.0000, 0.5399, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see the effect of dropout on a sample tensor. You can use the optional p argument to set the probability of an individual weight dropping out; if you dont it defaults to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions make deep learning possible. A neural network is really a program - with many parameters - that simulates a mathematical function. If all we did was multiple tensors by layer weights repeatedly, we could only simulate linear functions; further, there would be no point to having many layers, as the whole network would reduce could be reduced to a single matrix multiplication. Inserting non-linear activation functions between layers is what allows a deep learning model to simulate any function, rather than just linear ones.\n",
    "\n",
    "torch.nn.Module has objects encapsulating all of the major activation functions including ReLU and its many variants, Tanh, Hardtanh, sigmoid, and more. It also includes other functions, such as Softmax, that are most useful at the output stage of a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions tell us how far a models prediction is from the correct answer. PyTorch contains a variety of loss functions, including common MSE (mean squared error = L2 norm), Cross Entropy Loss and Negative Likelihood Loss (useful for classifiers), and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch TensorBoard Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before You Start\n",
    "To run this tutorial, youll need to install PyTorch, TorchVision, Matplotlib, and TensorBoard.\n",
    "\n",
    "With conda:\n",
    "conda install pytorch torchvision -c pytorch\n",
    "conda install matplotlib tensorboard\n",
    "\n",
    "With pip:\n",
    "pip install torch torchvision matplotlib tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, well be training a variant of LeNet-5 against the Fashion-MNIST dataset. Fashion-MNIST is a set of image tiles depicting various garments, with ten class labels indicating the type of garment depicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch model and training necessities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Image datasets and image manipulation\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Image display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# In case you are using an environment that has TensorFlow installed,\n",
    "# such as Google Colab, uncomment the following code to avoid\n",
    "# a bug with saving embeddings to your TensorBoard directory\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorboard as tb\n",
    "# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing Images in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by adding sample images from our dataset to TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather datasets and prepare them for consumption\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Store separate training and validations splits in ./data\n",
    "training_set = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                              batch_size=4,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=2)\n",
    "\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set,\n",
    "                                                batch_size=4,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=2)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# Extract a batch of 4 images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used TorchVision and Matplotlib to create a visual grid of a minibatch of our input data. Below, we use the add_image() call on SummaryWriter to log the image for consumption by TensorBoard, and we also call flush() to make sure its written to disk right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default log_dir argument is \"runs\" - but it's good to be specific\n",
    "# torch.utils.tensorboard.SummaryWriter is imported above\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')\n",
    "\n",
    "# Write image data to TensorBoard log dir\n",
    "writer.add_image('Four Fashion-MNIST Images', img_grid)\n",
    "writer.flush()\n",
    "\n",
    "# To view, start TensorBoard on the command line with:\n",
    "#   tensorboard --logdir=runs\n",
    "# ...and open a browser tab to http://localhost:6006/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start TensorBoard at the command line and open it in a new browser tab (usually at localhost:6006), you should see the image grid under the IMAGES tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing scalars to Visualize Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is useful for tracking the progress and efficacy of your training. Below, well run a training loop, track some metrics, and save the data for TensorBoards consumption.\n",
    "\n",
    "Lets define a model to categorize our image tiles, and an optimizer and loss function for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a single epocg, and evaluate the training vs. validation set losses every 1000 batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "Batch 1000\n",
      "Batch 2000\n",
      "Batch 3000\n",
      "Batch 4000\n",
      "Batch 5000\n",
      "Batch 6000\n",
      "Batch 7000\n",
      "Batch 8000\n",
      "Batch 9000\n",
      "Batch 10000\n",
      "Batch 11000\n",
      "Batch 12000\n",
      "Batch 13000\n",
      "Batch 14000\n",
      "Batch 15000\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_loader))\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        # basic training loop\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # Every 1000 mini-batches...\n",
    "            print('Batch {}'.format(i + 1))\n",
    "            # Check against the validation set\n",
    "            running_vloss = 0.0\n",
    "\n",
    "            # In evaluation mode some model specific operations can be omitted eg. dropout layer\n",
    "            net.train(False) # Switching to evaluation mode, eg. turning off regularisation\n",
    "            for j, vdata in enumerate(validation_loader, 0):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = net(vinputs)\n",
    "                vloss = criterion(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "            net.train(True) # Switching back to training mode, eg. turning on regularisation\n",
    "\n",
    "            avg_loss = running_loss / 1000\n",
    "            avg_vloss = running_vloss / len(validation_loader)\n",
    "\n",
    "            # Log the running loss averaged per batch\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                            epoch * len(training_loader) + i)\n",
    "\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "switch to your open TensorBoard and have a look at the SCALARS tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard can also be used to examine the data flow within your model. To do this, call the add_graph() method with a model and sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, grab a single mini-batch of images\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# add_graph() will trace the sample input through your model,\n",
    "# and render it as a graph.\n",
    "writer.add_graph(net, images)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you switch over to TensorBoard, you should see a GRAPHS tab. Double-click the NET node to see the layers and data flow within your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Your Dataset With Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 28-by-28 image tiles were using can be modeled as 784-dimensional vectors (28 * 28 = 784). It can be instructive to project this to a lower-dimensional representation. The add_embedding() method will project a set of data onto the three dimensions with highest variance, and display them as an interactive 3D chart. The add_embedding() method does this automatically by projecting to the three dimensions with highest variance.\n",
    "\n",
    "Below, well take a sample of our data, and generate such an embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random subset of data and corresponding labels\n",
    "def select_n_random(data, labels, n=100):\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# Extract a random subset of data\n",
    "images, labels = select_n_random(training_set.data, training_set.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[label] for label in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you switch to TensorBoard and select the PROJECTOR tab, you should see a 3D representation of the projection. You can rotate and zoom the model. Examine it at large and small scales, and see whether you can spot patterns in the projected data and the clustering of labels.\n",
    "\n",
    "For better visibility, its recommended to:\n",
    "\n",
    "Select label from the Color by drop-down on the left.\n",
    "\n",
    "Toggle the Night Mode icon along the top to place the light-colored images on a dark background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weve discussed and demonstrated:\n",
    "\n",
    "Building models with the neural network layers and functions of the torch.nn module\n",
    "\n",
    "The mechanics of automated gradient computation, which is central to gradient-based model training\n",
    "\n",
    "Using TensorBoard to visualize training progress and other activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, well be adding some new tools to your inventory:\n",
    "\n",
    "Well get familiar with the dataset and dataloader abstractions, and how they ease the process of feeding data to your model during a training loop\n",
    "\n",
    "Well discuss specific loss functions and when to use them\n",
    "\n",
    "Well look at PyTorch optimizers, which implement algorithms to adjust model weights based on the outcome of a loss function\n",
    "\n",
    "Finally, well pull all of these together and see a full PyTorch training loop in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset and DataLoader classes encapsulate the process of pulling your data from storage and exposing it to your training loop in batches.\n",
    "\n",
    "The Dataset is responsible for accessing and processing single instances of data.\n",
    "\n",
    "The DataLoader pulls instances of data from the Dataset (either automatically or with a sampler that you define), collects them in batches, and returns them for consumption by your training loop. The DataLoader works with all kinds of datasets, regardless of the type of data they contain.\n",
    "\n",
    "For this tutorial, well be using the Fashion-MNIST dataset provided by TorchVision. We use torchvision.transforms.Normalize() to zero-center and normalize the distribution of the image tile content, and download both training and validation data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as always, let's visualize the data as a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankle Boot  Dress  Sneaker  Pullover\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "The model we'll use in this example is a variant of LeNet-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class GarmentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GarmentClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GarmentClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, well be using a cross-entropy loss. For demonstration purposes, well create batches of dummy output and label values, run them through the loss function, and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1966, 0.7977, 0.5737, 0.1013, 0.7882, 0.1922, 0.6714, 0.8853, 0.9169,\n",
      "         0.1052],\n",
      "        [0.9903, 0.7030, 0.2487, 0.7148, 0.5824, 0.1114, 0.8666, 0.7997, 0.7809,\n",
      "         0.2232],\n",
      "        [0.9658, 0.6763, 0.4772, 0.7458, 0.1240, 0.7907, 0.8089, 0.8651, 0.5805,\n",
      "         0.5930],\n",
      "        [0.0895, 0.6302, 0.5365, 0.2451, 0.5431, 0.5948, 0.9977, 0.2078, 0.6073,\n",
      "         0.2784]])\n",
      "tensor([1, 5, 3, 7])\n",
      "Total loss for this batch: 2.4384467601776123\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# NB: Loss functions expect data in batches, so we're creating batches of 4\n",
    "# Represents the model's confidence in each of the 10 classes for a given input\n",
    "dummy_outputs = torch.rand(4, 10)\n",
    "# Represents the correct class among the 10 being tested\n",
    "dummy_labels = torch.tensor([1, 5, 3, 7])\n",
    "\n",
    "print(dummy_outputs)\n",
    "print(dummy_labels)\n",
    "\n",
    "loss = loss_fn(dummy_outputs, dummy_labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, well be using simple stochastic gradient descent with momentum.\n",
    "\n",
    "It can be instructive to try some variations on this optimization scheme:\n",
    "\n",
    "Learning rate determines the size of the steps the optimizer takes. What does a different learning rate do to the your training results, in terms of accuracy and convergence time?\n",
    "\n",
    "Momentum nudges the optimizer in the direction of strongest gradient over multiple steps. What does changing this value do to your results?\n",
    "\n",
    "Try some different optimization algorithms, such as averaged SGD, Adagrad, or Adam. How do your results differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have a function that performs one training epoch. It enumerates data from the DataLoader, and on each pass of the loop does the following:\n",
    "\n",
    "Gets a batch of training data from the DataLoader\n",
    "\n",
    "Zeros the optimizers gradients\n",
    "\n",
    "Performs an inference - that is, gets predictions from the model for an input batch\n",
    "\n",
    "Calculates the loss for that set of predictions vs. the labels on the dataset\n",
    "\n",
    "Calculates the backward gradients over the learning weights\n",
    "\n",
    "Tells the optimizer to perform one learning step - that is, adjust the models learning weights based on the observed gradients for this batch, according to the optimization algorithm we chose\n",
    "\n",
    "It reports on the loss for every 1000 batches.\n",
    "\n",
    "Finally, it reports the average per-batch loss for the last 1000 batches, for comparison with a validation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Epoch Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things well want to do once per epoch:\n",
    "\n",
    "Perform validation by checking our relative loss on a set of data that was not used for training, and report this\n",
    "\n",
    "Save a copy of the model\n",
    "\n",
    "Here, well do our reporting in TensorBoard. This will require going to the command line to start TensorBoard, and opening it in another browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 2.1717964600920676\n",
      "  batch 2000 loss: 0.9321752298176289\n",
      "  batch 3000 loss: 0.7051929750447161\n",
      "  batch 4000 loss: 0.6380287983976305\n",
      "  batch 5000 loss: 0.5825936196520924\n",
      "  batch 6000 loss: 0.5695326576875522\n",
      "  batch 7000 loss: 0.522997462703148\n",
      "  batch 8000 loss: 0.5104436654730234\n",
      "  batch 9000 loss: 0.49308474537244185\n",
      "  batch 10000 loss: 0.4952390701759141\n",
      "  batch 11000 loss: 0.4624113521499312\n",
      "  batch 12000 loss: 0.4444122429790441\n",
      "  batch 13000 loss: 0.4385216388797271\n",
      "  batch 14000 loss: 0.4278125356932869\n",
      "  batch 15000 loss: 0.39571767111937517\n",
      "LOSS train 0.39571767111937517 valid 0.4296651780605316\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.3822799964947626\n",
      "  batch 2000 loss: 0.3802146837072214\n",
      "  batch 3000 loss: 0.4082624550810433\n",
      "  batch 4000 loss: 0.3747677474583252\n",
      "  batch 5000 loss: 0.37915061244531534\n",
      "  batch 6000 loss: 0.37926533519421357\n",
      "  batch 7000 loss: 0.38140475272962066\n",
      "  batch 8000 loss: 0.38034670303331225\n",
      "  batch 9000 loss: 0.35105618538991257\n",
      "  batch 10000 loss: 0.3536376900322357\n",
      "  batch 11000 loss: 0.3842199795834313\n",
      "  batch 12000 loss: 0.3584070911887684\n",
      "  batch 13000 loss: 0.3468198613179702\n",
      "  batch 14000 loss: 0.3423277017622022\n",
      "  batch 15000 loss: 0.33989165431653967\n",
      "LOSS train 0.33989165431653967 valid 0.3730338513851166\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.3240579486264687\n",
      "  batch 2000 loss: 0.29625674850774886\n",
      "  batch 3000 loss: 0.33788437662295656\n",
      "  batch 4000 loss: 0.3410449398945784\n",
      "  batch 5000 loss: 0.31483021319775434\n",
      "  batch 6000 loss: 0.3189915870522091\n",
      "  batch 7000 loss: 0.3254711688522802\n",
      "  batch 8000 loss: 0.3195269274674065\n",
      "  batch 9000 loss: 0.3494923338386288\n",
      "  batch 10000 loss: 0.2977668858261859\n",
      "  batch 11000 loss: 0.3330979030437884\n",
      "  batch 12000 loss: 0.3276756107229885\n",
      "  batch 13000 loss: 0.33154238461425123\n",
      "  batch 14000 loss: 0.3216811506494705\n",
      "  batch 15000 loss: 0.3072162611018539\n",
      "LOSS train 0.3072162611018539 valid 0.343028724193573\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.2974540507921483\n",
      "  batch 2000 loss: 0.30270247245451176\n",
      "  batch 3000 loss: 0.29229248701384314\n",
      "  batch 4000 loss: 0.3090817196467542\n",
      "  batch 5000 loss: 0.30420187833183443\n",
      "  batch 6000 loss: 0.30347143759990286\n",
      "  batch 7000 loss: 0.2775786739559262\n",
      "  batch 8000 loss: 0.30045182189167463\n",
      "  batch 9000 loss: 0.2772500331264127\n",
      "  batch 10000 loss: 0.31653565505229925\n",
      "  batch 11000 loss: 0.28738344668938587\n",
      "  batch 12000 loss: 0.28711608100853847\n",
      "  batch 13000 loss: 0.3011898504185319\n",
      "  batch 14000 loss: 0.2755644448480648\n",
      "  batch 15000 loss: 0.2801266105116374\n",
      "LOSS train 0.2801266105116374 valid 0.34984081983566284\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.2795910517865559\n",
      "  batch 2000 loss: 0.2605563573783612\n",
      "  batch 3000 loss: 0.288174892666951\n",
      "  batch 4000 loss: 0.26336117389161884\n",
      "  batch 5000 loss: 0.25973076217775815\n",
      "  batch 6000 loss: 0.2783609585749073\n",
      "  batch 7000 loss: 0.2695847715082709\n",
      "  batch 8000 loss: 0.275955986120307\n",
      "  batch 9000 loss: 0.27468236799282386\n",
      "  batch 10000 loss: 0.27597380809986133\n",
      "  batch 11000 loss: 0.2713258398458108\n",
      "  batch 12000 loss: 0.28215809422526944\n",
      "  batch 13000 loss: 0.2824828548624937\n",
      "  batch 14000 loss: 0.27731230519036765\n",
      "  batch 15000 loss: 0.2924090956987202\n",
      "LOSS train 0.2924090956987202 valid 0.31975841522216797\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a saved version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GarmentClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m GarmentClassifier()\n\u001b[1;32m      2\u001b[0m saved_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_20240922_083248_0\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GarmentClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "saved_model = GarmentClassifier()\n",
    "saved_model.load_state_dict(torch.load(\"model_20240922_083248_0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once youve loaded the model, its ready for whatever you need it for - more training, inference, or analysis.\n",
    "\n",
    "Note that if your model has constructor parameters that affect model structure, youll need to provide them and configure the model identically to the state in which it was saved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Understanding with Captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A First Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, lets take a simple, visual example. Well start with a ResNet model pretrained on the ImageNet dataset. Well get a test input, and use different Feature Attribution algorithms to examine how the input images affect the output, and see a helpful visualization of this input attribution map for some test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, some imports:\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now well use the TorchVision model library to download a pretrained ResNet. Since were not training, well place it in evaluation mode for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/anjalisuman/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|| 44.7M/44.7M [00:14<00:00, 3.16MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The place where you got this interactive notebook should also have an img folder with a file cat.jpg in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/anjalisuman/Documents/GitHub/ML101/img/cat.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg/cat.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m test_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/correct/path/to/cat.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#test_img = Image.open('/Users/anjalisuman/Documents/GitHub/ML101/img/cat.jpg')\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/PIL/Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/anjalisuman/Documents/GitHub/ML101/img/cat.jpg'"
     ]
    }
   ],
   "source": [
    "test_img = Image.open('img/cat.jpg')\n",
    "test_img = Image.open('/correct/path/to/cat.jpg')\n",
    "#test_img = Image.open('/Users/anjalisuman/Documents/GitHub/ML101/img/cat.jpg')\n",
    "\n",
    "test_img_data = np.asarray(test_img)\n",
    "plt.imshow(test_img_data)\n",
    "plt.show()\n",
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our ResNet model was trained on the ImageNet dataset, and expects images to be of a certain size, with the channel data normalized to a specific range of values. Well also pull in the list of human-readable labels for the categories our model recognizes - that should be in the img folder as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# standard ImageNet normalization\u001b[39;00m\n\u001b[1;32m      9\u001b[0m transform_normalize \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[1;32m     10\u001b[0m      mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],\n\u001b[1;32m     11\u001b[0m      std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]\n\u001b[1;32m     12\u001b[0m  )\n\u001b[0;32m---> 14\u001b[0m transformed_img \u001b[38;5;241m=\u001b[39m transform(test_img)\n\u001b[1;32m     15\u001b[0m input_img \u001b[38;5;241m=\u001b[39m transform_normalize(transformed_img)\n\u001b[1;32m     16\u001b[0m input_img \u001b[38;5;241m=\u001b[39m input_img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# the model requires a dummy batch dimension\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_img' is not defined"
     ]
    }
   ],
   "source": [
    "# model expects 224x224 3-color image\n",
    "transform = transforms.Compose([\n",
    " transforms.Resize(224),\n",
    " transforms.CenterCrop(224),\n",
    " transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# standard ImageNet normalization\n",
    "transform_normalize = transforms.Normalize(\n",
    "     mean=[0.485, 0.456, 0.406],\n",
    "     std=[0.229, 0.224, 0.225]\n",
    " )\n",
    "\n",
    "transformed_img = transform(test_img)\n",
    "input_img = transform_normalize(transformed_img)\n",
    "input_img = input_img.unsqueeze(0) # the model requires a dummy batch dimension\n",
    "\n",
    "labels_path = 'img/imagenet_class_index.json'\n",
    "with open(labels_path) as json_data:\n",
    "    idx_to_labels = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can ask the question: What does our model think this image represents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_img)\n\u001b[1;32m      2\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m prediction_score, pred_label_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(output, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_img' is not defined"
     ]
    }
   ],
   "source": [
    "output = model(input_img)\n",
    "output = F.softmax(output, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "pred_label_idx.squeeze_()\n",
    "predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "print('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weve confirmed that ResNet thinks our image of a cat is, in fact, a cat. But why does the model think this is an image of a cat?\n",
    "\n",
    "For the answer to that, we turn to Captum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Attribution with Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature attribution attributes a particular output to features of the input. It uses a specific input - here, our test image - to generate a map of the relative importance of each input feature to a particular output feature.\n",
    "\n",
    "Integrated Gradients is one of the feature attribution algorithms available in Captum. Integrated Gradients assigns an importance score to each input feature by approximating the integral of the gradients of the models output with respect to the inputs.\n",
    "\n",
    "In our case, were going to be taking a specific element of the output vector - that is, the one indicating the models confidence in its chosen category - and use Integrated Gradients to understand what parts of the input image contributed to this output.\n",
    "\n",
    "Once we have the importance map from Integrated Gradients, well use the visualization tools in Captum to give a helpful representation of the importance map. Captums visualize_image_attr() function provides a variety of options for customizing display of your attribution data. Here, we pass in a custom Matplotlib color map.\n",
    "\n",
    "Running the cell with the integrated_gradients.attribute() call will usually take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m integrated_gradients \u001b[38;5;241m=\u001b[39m IntegratedGradients(model)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Ask the algorithm to attribute our output target to\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m attributions_ig \u001b[38;5;241m=\u001b[39m integrated_gradients\u001b[38;5;241m.\u001b[39mattribute(input_img, target\u001b[38;5;241m=\u001b[39mpred_label_idx, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Show the original image for comparison\u001b[39;00m\n\u001b[1;32m      8\u001b[0m _ \u001b[38;5;241m=\u001b[39m viz\u001b[38;5;241m.\u001b[39mvisualize_image_attr(\u001b[38;5;28;01mNone\u001b[39;00m, np\u001b[38;5;241m.\u001b[39mtranspose(transformed_img\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)),\n\u001b[1;32m      9\u001b[0m                       method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_image\u001b[39m\u001b[38;5;124m\"\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_img' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the attribution algorithm with the model\n",
    "integrated_gradients = IntegratedGradients(model)\n",
    "\n",
    "# Ask the algorithm to attribute our output target to\n",
    "attributions_ig = integrated_gradients.attribute(input_img, target=pred_label_idx, n_steps=200)\n",
    "\n",
    "# Show the original image for comparison\n",
    "_ = viz.visualize_image_attr(None, np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                      method=\"original_image\", title=\"Original Image\")\n",
    "\n",
    "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#0000ff'),\n",
    "                                                  (1, '#0000ff')], N=256)\n",
    "\n",
    "_ = viz.visualize_image_attr(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                             np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                             method='heat_map',\n",
    "                             cmap=default_cmap,\n",
    "                             show_colorbar=True,\n",
    "                             sign='positive',\n",
    "                             title='Integrated Gradients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n the image above, you should see that Integrated Gradients gives us the strongest signal around the cats location in the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Attribution with Occlusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-based attribution methods help to understand the model in terms of directly computing out the output changes with respect to the input. Perturbation-based attribution methods approach this more directly, by introducing changes to the input to measure the effect on the output. Occlusion is one such method. It involves replacing sections of the input image, and examining the effect on the output signal.\n",
    "\n",
    "Below, we set up Occlusion attribution. Similarly to configuring a convolutional neural network, you can specify the size of the target region, and a stride length to determine the spacing of individual measurements. Well visualize the output of our Occlusion attribution with visualize_image_attr_multiple(), showing heat maps of both positive and negative attribution by region, and by masking the original image with the positive attribution regions. The masking gives a very instructive view of what regions of our cat photo the model found to be most cat-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m occlusion \u001b[38;5;241m=\u001b[39m Occlusion(model)\n\u001b[0;32m----> 3\u001b[0m attributions_occ \u001b[38;5;241m=\u001b[39m occlusion\u001b[38;5;241m.\u001b[39mattribute(input_img,\n\u001b[1;32m      4\u001b[0m                                        target\u001b[38;5;241m=\u001b[39mpred_label_idx,\n\u001b[1;32m      5\u001b[0m                                        strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m      6\u001b[0m                                        sliding_window_shapes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m),\n\u001b[1;32m      7\u001b[0m                                        baselines\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m _ \u001b[38;5;241m=\u001b[39m viz\u001b[38;5;241m.\u001b[39mvisualize_image_attr_multiple(np\u001b[38;5;241m.\u001b[39mtranspose(attributions_occ\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)),\n\u001b[1;32m     11\u001b[0m                                       np\u001b[38;5;241m.\u001b[39mtranspose(transformed_img\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)),\n\u001b[1;32m     12\u001b[0m                                       [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_image\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheat_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheat_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasked_image\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                       fig_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m     17\u001b[0m                                      )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_img' is not defined"
     ]
    }
   ],
   "source": [
    "occlusion = Occlusion(model)\n",
    "\n",
    "attributions_occ = occlusion.attribute(input_img,\n",
    "                                       target=pred_label_idx,\n",
    "                                       strides=(3, 8, 8),\n",
    "                                       sliding_window_shapes=(3,15, 15),\n",
    "                                       baselines=0)\n",
    "\n",
    "\n",
    "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                      [\"original_image\", \"heat_map\", \"heat_map\", \"masked_image\"],\n",
    "                                      [\"all\", \"positive\", \"negative\", \"positive\"],\n",
    "                                      show_colorbar=True,\n",
    "                                      titles=[\"Original\", \"Positive Attribution\", \"Negative Attribution\", \"Masked\"],\n",
    "                                      fig_size=(18, 6)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see greater significance placed on the region of the image that contains the cat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Attribution with Layer GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Attribution allows you to attribute the activity of hidden layers within your model to features of your input. Below, well use a layer attribution algorithm to examine the activity of one of the convolutional layers within our model.\n",
    "\n",
    "GradCAM computes the gradients of the target output with respect to the given layer, averages for each output channel (dimension 2 of output), and multiplies the average gradient for each channel by the layer activations. The results are summed over all channels. GradCAM is designed for convnets; since the activity of convolutional layers often maps spatially to the input, GradCAM attributions are often upsampled and used to mask the input.\n",
    "\n",
    "Layer attribution is set up similarly to input attribution, except that in addition to the model, you must specify a hidden layer within the model that you wish to examine. As above, when we call attribute(), we specify the target class of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m layer_gradcam \u001b[38;5;241m=\u001b[39m LayerGradCam(model, model\u001b[38;5;241m.\u001b[39mlayer3[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mconv2)\n\u001b[0;32m----> 2\u001b[0m attributions_lgc \u001b[38;5;241m=\u001b[39m layer_gradcam\u001b[38;5;241m.\u001b[39mattribute(input_img, target\u001b[38;5;241m=\u001b[39mpred_label_idx)\n\u001b[1;32m      4\u001b[0m _ \u001b[38;5;241m=\u001b[39m viz\u001b[38;5;241m.\u001b[39mvisualize_image_attr(attributions_lgc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m      5\u001b[0m                              sign\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                              title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer 3 Block 1 Conv 2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_img' is not defined"
     ]
    }
   ],
   "source": [
    "layer_gradcam = LayerGradCam(model, model.layer3[1].conv2)\n",
    "attributions_lgc = layer_gradcam.attribute(input_img, target=pred_label_idx)\n",
    "\n",
    "_ = viz.visualize_image_attr(attributions_lgc[0].cpu().permute(1,2,0).detach().numpy(),\n",
    "                             sign=\"all\",\n",
    "                             title=\"Layer 3 Block 1 Conv 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well use the convenience method interpolate() in the LayerAttribution base class to upsample this attribution data for comparison to the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attributions_lgc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m upsamp_attr_lgc \u001b[38;5;241m=\u001b[39m LayerAttribution\u001b[38;5;241m.\u001b[39minterpolate(attributions_lgc, input_img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(attributions_lgc\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(upsamp_attr_lgc\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attributions_lgc' is not defined"
     ]
    }
   ],
   "source": [
    "upsamp_attr_lgc = LayerAttribution.interpolate(attributions_lgc, input_img.shape[2:])\n",
    "\n",
    "print(attributions_lgc.shape)\n",
    "print(upsamp_attr_lgc.shape)\n",
    "print(input_img.shape)\n",
    "\n",
    "_ = viz.visualize_image_attr_multiple(upsamp_attr_lgc[0].cpu().permute(1,2,0).detach().numpy(),\n",
    "                                      transformed_img.permute(1,2,0).numpy(),\n",
    "                                      [\"original_image\",\"blended_heat_map\",\"masked_image\"],\n",
    "                                      [\"all\",\"positive\",\"positive\"],\n",
    "                                      show_colorbar=True,\n",
    "                                      titles=[\"Original\", \"Positive Attribution\", \"Masked\"],\n",
    "                                      fig_size=(18, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations such as this can give you novel insights into how your hidden layers respond to your input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with Captum Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captum Insights is an interpretability visualization widget built on top of Captum to facilitate model understanding. Captum Insights works across images, text, and other features to help users understand feature attribution. It allows you to visualize attribution for multiple input/output pairs, and provides visualization tools for image, text, and arbitrary data.\n",
    "\n",
    "In this section of the notebook, well visualize multiple image classification inferences with Captum Insights.\n",
    "\n",
    "First, lets gather some image and see what the model thinks of them. For variety, well take our cat, a teapot, and a trilobite fossil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/anjalisuman/Documents/GitHub/ML101/img/cat.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m imgs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg/cat.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg/teapot.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg/trilobite.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs:\n\u001b[0;32m----> 4\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img)\n\u001b[1;32m      5\u001b[0m     transformed_img \u001b[38;5;241m=\u001b[39m transform(img)\n\u001b[1;32m      6\u001b[0m     input_img \u001b[38;5;241m=\u001b[39m transform_normalize(transformed_img)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/PIL/Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/anjalisuman/Documents/GitHub/ML101/img/cat.jpg'"
     ]
    }
   ],
   "source": [
    "imgs = ['img/cat.jpg', 'img/teapot.jpg', 'img/trilobite.jpg']\n",
    "\n",
    "for img in imgs:\n",
    "    img = Image.open(img)\n",
    "    transformed_img = transform(img)\n",
    "    input_img = transform_normalize(transformed_img)\n",
    "    input_img = input_img.unsqueeze(0) # the model requires a dummy batch dimension\n",
    "\n",
    "    output = model(input_img)\n",
    "    output = F.softmax(output, dim=1)\n",
    "    prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "    pred_label_idx.squeeze_()\n",
    "    predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n",
    "    print('Predicted:', predicted_label, '/', pred_label_idx.item(), ' (', prediction_score.squeeze().item(), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and it looks like our model is identifying them all correctly - but of course, we want to dig deeper. For that well use the Captum Insights widget, which we configure with an AttributionVisualizer object, imported below. The AttributionVisualizer expects batches of data, so well bring in Captums Batch helper class. And well be looking at images specifically, so well also import ImageFeature.\n",
    "\n",
    "We configure the AttributionVisualizer with the following arguments:\n",
    "\n",
    "An array of models to be examined (in our case, just the one)\n",
    "\n",
    "A scoring function, which allows Captum Insights to pull out the top-k predictions from a model\n",
    "\n",
    "An ordered, human-readable list of classes our model is trained on\n",
    "\n",
    "A list of features to look for - in our case, an ImageFeature\n",
    "\n",
    "A dataset, which is an iterable object returning batches of inputs and labels - just like youd use for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask_compress'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minsights\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributionVisualizer, Batch\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minsights\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr_vis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFeature\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Baseline is all-zeros input - this may differ depending on your data\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/captum/insights/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minsights\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr_vis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributionVisualizer, Batch, features\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/captum/insights/attr_vis/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minsights\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr_vis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributionVisualizer, Batch\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/captum/insights/attr_vis/app.py:28\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minsights\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr_vis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     ATTRIBUTION_METHOD_CONFIG,\n\u001b[1;32m     25\u001b[0m     ATTRIBUTION_NAMES_TO_METHODS,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minsights\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr_vis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseFeature\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minsights\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattr_vis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple_to_dict\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log_usage\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/captum/insights/attr_vis/server.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaptum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log_usage\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flask, jsonify, render_template, request\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask_compress\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Compress\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     14\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;18m__name__\u001b[39m, static_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrontend/build/static\u001b[39m\u001b[38;5;124m\"\u001b[39m, template_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrontend/build\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flask_compress'"
     ]
    }
   ],
   "source": [
    "from captum.insights import AttributionVisualizer, Batch\n",
    "from captum.insights.attr_vis.features import ImageFeature\n",
    "\n",
    "# Baseline is all-zeros input - this may differ depending on your data\n",
    "def baseline_func(input):\n",
    "    return input * 0\n",
    "\n",
    "# merging our image transforms from above\n",
    "def full_img_transform(input):\n",
    "    i = Image.open(input)\n",
    "    i = transform(i)\n",
    "    i = transform_normalize(i)\n",
    "    i = i.unsqueeze(0)\n",
    "    return i\n",
    "\n",
    "\n",
    "input_imgs = torch.cat(list(map(lambda i: full_img_transform(i), imgs)), 0)\n",
    "\n",
    "visualizer = AttributionVisualizer(\n",
    "    models=[model],\n",
    "    score_func=lambda o: torch.nn.functional.softmax(o, 1),\n",
    "    classes=list(map(lambda k: idx_to_labels[k][1], idx_to_labels.keys())),\n",
    "    features=[\n",
    "        ImageFeature(\n",
    "            \"Photo\",\n",
    "            baseline_transforms=[baseline_func],\n",
    "            input_transforms=[],\n",
    "        )\n",
    "    ],\n",
    "    dataset=[Batch(input_imgs, labels=[282,849,69])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that running the cell above didnt take much time at all, unlike our attributions above. Thats because Captum Insights lets you configure different attribution algorithms in a visual widget, after which it will compute and display the attributions. That process will take a few minutes.\n",
    "\n",
    "Running the cell below will render the Captum Insights widget. You can then choose attributions methods and their arguments, filter model responses based on predicted class or prediction correctness, see the models predictions with associated probabilities, and view heatmaps of the attribution compared with the original image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m visualizer\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'visualizer' is not defined"
     ]
    }
   ],
   "source": [
    "visualizer.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
